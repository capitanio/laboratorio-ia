<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Elaborazione del Linguaggio Naturale – Laboratorio di Intelligenza Artificiale</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bibliografia.html" rel="next">
<link href="./3-6-deep-learning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-254d8caa02a4f51d576d86802a86f2db.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e5911a59318b73639a72866017db9c42.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-254d8caa02a4f51d576d86802a86f2db.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="epub.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-machine learning.html">Machine learning</a></li><li class="breadcrumb-item"><a href="./3-7-elaborazione-linguaggio-naturale.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Elaborazione del Linguaggio Naturale</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Laboratorio di Intelligenza Artificiale</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Prefazione dell’autore{.unnumbered}</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-introduzione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Turing-vs-Searle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Turing vs Searle</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./2-algoritmi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algoritmi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-1-inferenza-logica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Inferenza Logica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-2-inferenza-probabilistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inferenza Probabilistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-3-inferenza-bayesiana.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inferenza Bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-4-algoritmi-di-ricerca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Algoritmi di Ricerca</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-5-algoritmi-equitativi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Algoritmi Equitativi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-6-algoritmi-predittivi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Algoritmi Predittivi</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./3-machine learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-1-apprendimento-supervisionato.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Apprendimento Supervisionato</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-2-apprendimento-non-supervisionato.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Apprendimento Non Supervisionato</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-3-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Bias</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-4-regressione-lineare-e-logistica.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Regressione Lineare e Logistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-5-perceptrone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Percetptrone</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-6-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Deep learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-7-elaborazione-linguaggio-naturale.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Elaborazione del Linguaggio Naturale</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bibliografia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bibliografia</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduzione" id="toc-introduzione" class="nav-link active" data-scroll-target="#introduzione"><span class="header-section-number">15.1</span> Introduzione</a></li>
  <li><a href="#obiettivi-dellnlp" id="toc-obiettivi-dellnlp" class="nav-link" data-scroll-target="#obiettivi-dellnlp"><span class="header-section-number">15.2</span> Obiettivi dell’NLP</a></li>
  <li><a href="#acquisizione" id="toc-acquisizione" class="nav-link" data-scroll-target="#acquisizione"><span class="header-section-number">15.3</span> Acquisizione</a></li>
  <li><a href="#pre-elaborazione" id="toc-pre-elaborazione" class="nav-link" data-scroll-target="#pre-elaborazione"><span class="header-section-number">15.4</span> Pre-elaborazione</a></li>
  <li><a href="#word-embeddings" id="toc-word-embeddings" class="nav-link" data-scroll-target="#word-embeddings"><span class="header-section-number">15.5</span> Word Embeddings</a>
  <ul class="collapse">
  <li><a href="#proprietà-sintattiche-dei-word-embeddings" id="toc-proprietà-sintattiche-dei-word-embeddings" class="nav-link" data-scroll-target="#proprietà-sintattiche-dei-word-embeddings"><span class="header-section-number">15.5.1</span> Proprietà Sintattiche dei Word Embeddings</a></li>
  <li><a href="#proprietà-semantiche-dei-word-embeddings" id="toc-proprietà-semantiche-dei-word-embeddings" class="nav-link" data-scroll-target="#proprietà-semantiche-dei-word-embeddings"><span class="header-section-number">15.5.2</span> Proprietà Semantiche dei Word Embeddings</a></li>
  <li><a href="#tecniche-di-generazione" id="toc-tecniche-di-generazione" class="nav-link" data-scroll-target="#tecniche-di-generazione"><span class="header-section-number">15.5.3</span> Tecniche di Generazione</a></li>
  </ul></li>
  <li><a href="#sentence-embeddings" id="toc-sentence-embeddings" class="nav-link" data-scroll-target="#sentence-embeddings"><span class="header-section-number">15.6</span> Sentence Embeddings</a>
  <ul class="collapse">
  <li><a href="#proprietà-dei-sentence-embeddings" id="toc-proprietà-dei-sentence-embeddings" class="nav-link" data-scroll-target="#proprietà-dei-sentence-embeddings"><span class="header-section-number">15.6.1</span> Proprietà dei Sentence Embeddings</a></li>
  <li><a href="#applicazioni" id="toc-applicazioni" class="nav-link" data-scroll-target="#applicazioni"><span class="header-section-number">15.6.2</span> Applicazioni</a></li>
  <li><a href="#tecniche-per-la-generazione-di-sentence-embeddings" id="toc-tecniche-per-la-generazione-di-sentence-embeddings" class="nav-link" data-scroll-target="#tecniche-per-la-generazione-di-sentence-embeddings"><span class="header-section-number">15.6.3</span> Tecniche per la Generazione di Sentence Embeddings</a></li>
  </ul></li>
  <li><a href="#transformer" id="toc-transformer" class="nav-link" data-scroll-target="#transformer"><span class="header-section-number">15.7</span> Transformer</a>
  <ul class="collapse">
  <li><a href="#il-cuore-dei-transformer-lattenzione" id="toc-il-cuore-dei-transformer-lattenzione" class="nav-link" data-scroll-target="#il-cuore-dei-transformer-lattenzione"><span class="header-section-number">15.7.1</span> Il Cuore dei Transformer: l’attenzione</a></li>
  <li><a href="#multi-head-attention-vedere-il-linguaggio-da-diverse-prospettive" id="toc-multi-head-attention-vedere-il-linguaggio-da-diverse-prospettive" class="nav-link" data-scroll-target="#multi-head-attention-vedere-il-linguaggio-da-diverse-prospettive"><span class="header-section-number">15.7.2</span> Multi-Head Attention: Vedere il Linguaggio da Diverse Prospettive</a></li>
  <li><a href="#larchitettura-del-transformer-encoder-e-decoder" id="toc-larchitettura-del-transformer-encoder-e-decoder" class="nav-link" data-scroll-target="#larchitettura-del-transformer-encoder-e-decoder"><span class="header-section-number">15.7.3</span> L’Architettura del Transformer: Encoder e Decoder</a></li>
  <li><a href="#perché-i-transformer-hanno-rivoluzionato-lnlp" id="toc-perché-i-transformer-hanno-rivoluzionato-lnlp" class="nav-link" data-scroll-target="#perché-i-transformer-hanno-rivoluzionato-lnlp"><span class="header-section-number">15.7.4</span> Perché i Transformer Hanno Rivoluzionato l’NLP?</a></li>
  </ul></li>
  <li><a href="#large-language-models-llm" id="toc-large-language-models-llm" class="nav-link" data-scroll-target="#large-language-models-llm"><span class="header-section-number">15.8</span> Large Language Models (LLM)</a>
  <ul class="collapse">
  <li><a href="#prompt-engineering" id="toc-prompt-engineering" class="nav-link" data-scroll-target="#prompt-engineering"><span class="header-section-number">15.8.1</span> Prompt Engineering</a></li>
  <li><a href="#modelli-llm-locali" id="toc-modelli-llm-locali" class="nav-link" data-scroll-target="#modelli-llm-locali"><span class="header-section-number">15.8.2</span> Modelli LLM “Locali”</a></li>
  </ul></li>
  <li><a href="#applicazioni-in-giurisprudenza" id="toc-applicazioni-in-giurisprudenza" class="nav-link" data-scroll-target="#applicazioni-in-giurisprudenza"><span class="header-section-number">15.9</span> Applicazioni in Giurisprudenza</a></li>
  <li><a href="#sfide-e-considerazioni-etiche" id="toc-sfide-e-considerazioni-etiche" class="nav-link" data-scroll-target="#sfide-e-considerazioni-etiche"><span class="header-section-number">15.10</span> Sfide e Considerazioni Etiche</a></li>
  <li><a href="#conclusioni" id="toc-conclusioni" class="nav-link" data-scroll-target="#conclusioni"><span class="header-section-number">15.11</span> Conclusioni</a></li>
  <li><a href="#laboratorio-di-python" id="toc-laboratorio-di-python" class="nav-link" data-scroll-target="#laboratorio-di-python"><span class="header-section-number">15.12</span> Laboratorio di Python</a>
  <ul class="collapse">
  <li><a href="#esperimento-1-introduzione-ai-word-embeddings" id="toc-esperimento-1-introduzione-ai-word-embeddings" class="nav-link" data-scroll-target="#esperimento-1-introduzione-ai-word-embeddings"><span class="header-section-number">15.12.1</span> Esperimento 1: Introduzione ai Word Embeddings</a></li>
  <li><a href="#esperimento-2-analisi-del-sentiment" id="toc-esperimento-2-analisi-del-sentiment" class="nav-link" data-scroll-target="#esperimento-2-analisi-del-sentiment"><span class="header-section-number">15.12.2</span> Esperimento 2: Analisi del Sentiment</a></li>
  <li><a href="#esperimento-3-sistema-di-domanda-risposta-su-un-testo-giuridico" id="toc-esperimento-3-sistema-di-domanda-risposta-su-un-testo-giuridico" class="nav-link" data-scroll-target="#esperimento-3-sistema-di-domanda-risposta-su-un-testo-giuridico"><span class="header-section-number">15.12.3</span> Esperimento 3: Sistema di Domanda-Risposta su un Testo Giuridico</a></li>
  </ul></li>
  <li><a href="#esercizi" id="toc-esercizi" class="nav-link" data-scroll-target="#esercizi"><span class="header-section-number">15.13</span> Esercizi</a>
  <ul class="collapse">
  <li><a href="#esercizio-1-pre-elaborazione-di-un-testo-giuridico" id="toc-esercizio-1-pre-elaborazione-di-un-testo-giuridico" class="nav-link" data-scroll-target="#esercizio-1-pre-elaborazione-di-un-testo-giuridico"><span class="header-section-number">15.13.1</span> Esercizio 1: Pre-elaborazione di un Testo Giuridico</a></li>
  <li><a href="#esercizio-2-creazione-di-un-prompt-efficace" id="toc-esercizio-2-creazione-di-un-prompt-efficace" class="nav-link" data-scroll-target="#esercizio-2-creazione-di-un-prompt-efficace"><span class="header-section-number">15.13.2</span> Esercizio 2: Creazione di un prompt efficace</a></li>
  <li><a href="#esercizio-3-analisi-semantica-con-word-embeddings" id="toc-esercizio-3-analisi-semantica-con-word-embeddings" class="nav-link" data-scroll-target="#esercizio-3-analisi-semantica-con-word-embeddings"><span class="header-section-number">15.13.3</span> Esercizio 3: Analisi semantica con word embeddings</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-machine learning.html">Machine learning</a></li><li class="breadcrumb-item"><a href="./3-7-elaborazione-linguaggio-naturale.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Elaborazione del Linguaggio Naturale</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-nlp" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Elaborazione del Linguaggio Naturale</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduzione" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="introduzione"><span class="header-section-number">15.1</span> Introduzione</h2>
<p>Il linguaggio naturale è la lingua parlata o scritta che usiamo quotidianamente per comunicare. È ricco, complesso e carico di ambiguità. Un frammento di testo può contenere riferimenti impliciti, sinonimi, metafore e strutture grammaticali variabili. Ad esempio, la frase “Marta ha visto Luca in giardino con il binocolo” è ambigua: chi osserva con il binocolo? Chi è osservato con il binocolo? Per un sistema NLP, risolvere questa ambiguità richiede di analizzare il contesto o utilizzare modelli avanzati come i <em>Transformer</em> <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>.</p>
</section>
<section id="obiettivi-dellnlp" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="obiettivi-dellnlp"><span class="header-section-number">15.2</span> Obiettivi dell’NLP</h2>
<p>L’elaborazione del linguaggio naturale (NLP) è un processo che acquisisce una porzione di testo, chiamata <em>documento</em> (anche se si tratta di una parola o una frase), la scompone in unità elementari dette <em>token</em> e procede con fasi di analisi, comprensione e generazione. Ad esempio, in ambito giuridico, l’NLP può estrarre clausole chiave da un contratto o generare riassunti di sentenze <span class="citation" data-cites="surden2019">(<a href="#ref-surden2019" role="doc-biblioref">Surden 2019</a>)</span>.</p>
</section>
<section id="acquisizione" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="acquisizione"><span class="header-section-number">15.3</span> Acquisizione</h2>
<p>L’acquisizione del documento inizia con la raccolta dei dati, che possono essere testuali o audio. I dati testuali sono codificati in sequenze di caratteri, mentre i dati audio sono trasformati in campioni digitali e convertiti in testo. Al termine, il documento è pronto per l’elaborazione, memorizzabile in un file di testo o in un database.</p>
</section>
<section id="pre-elaborazione" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="pre-elaborazione"><span class="header-section-number">15.4</span> Pre-elaborazione</h2>
<p>La <em>pre-elaborazione</em> è fondamentale per preparare i dati testuali all’analisi, riducendo complessità, ridondanza e variabilità del linguaggio naturale. Serve a:</p>
<ul>
<li><em>Riduzione della complessità</em>: testi più semplici per modelli più efficienti.</li>
<li><em>Migliore rappresentazione</em>: vettori numerici più rappresentativi del significato.</li>
<li><em>Migliore generalizzazione</em>: evita che i modelli apprendano rumore linguistico.</li>
</ul>
<p>Le principali operazioni sono:</p>
<ul>
<li><em>Tokenizzazione</em>: suddivide il testo in <em>token</em> (parole, frasi o caratteri).<br>
<em>Esempio</em>: “La giustizia è uguale per tutti.” → [“La”, “giustizia”, “è”, “uguale”, “per”, “tutti”, “.”]</li>
<li><em>Rimozione della punteggiatura</em>: elimina segni non utili all’analisi.</li>
<li><em>Normalizzazione del testo</em>: standardizza le parole, ad esempio:
<ul>
<li>Conversione in minuscolo.<br>
</li>
<li>Espansione di forme contratte (“don’t” → “do not”).</li>
</ul></li>
<li><em>Rimozione delle stopword</em>: elimina parole frequenti ma poco rilevanti (“il”, “e”, “di”).</li>
<li><em>Stemming e lemmatizzazione</em>:
<ul>
<li><em>Stemming</em>: riduce una parola alla radice (“giudicando” → “giudic”).<br>
</li>
<li><em>Lemmatizzazione</em>: riduce alla forma base, considerando il contesto (“giudicando” → “giudicare”).</li>
</ul></li>
<li><em>Rimozione di cifre o caratteri speciali</em>: utile se non rilevanti.</li>
<li><em>Correzione ortografica</em> (facoltativa): migliora la qualità dei dati.</li>
</ul>
<p>La scelta delle tecniche dipende dall’applicazione: per analisi giuridiche o traduzioni, la <em>lemmatizzazione</em> è preferibile; per compiti generali, basta lo <em>stemming</em>. Al termine, si ottiene un documento di <em>token</em> o parole, pronti per essere convertiti in numeri.</p>
</section>
<section id="word-embeddings" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="word-embeddings"><span class="header-section-number">15.5</span> Word Embeddings</h2>
<p>I <em>word embeddings</em> convertono le parole in vettori numerici per l’elaborazione computazionale. Questi vettori, in uno spazio multidimensionale, catturano le relazioni tra parole in modo compatto ed efficiente, consentendo l’applicazione di algoritmi di machine learning e deep learning <span class="citation" data-cites="mikolov2013wordrepresentations">(<a href="#ref-mikolov2013wordrepresentations" role="doc-biblioref">Mikolov et al. 2013</a>)</span>. Sono essenziali per attività come l’analisi del sentiment, la classificazione del testo, la traduzione, il riconoscimento di entità nominate e il <em>question answering</em>. Negli ultimi anni, numerosi articoli scientifici hanno esplorato questo tema (<a href="https://www.semanticscholar.org">Semanticscholar.org</a>).</p>
<section id="proprietà-sintattiche-dei-word-embeddings" class="level3" data-number="15.5.1">
<h3 data-number="15.5.1" class="anchored" data-anchor-id="proprietà-sintattiche-dei-word-embeddings"><span class="header-section-number">15.5.1</span> Proprietà Sintattiche dei Word Embeddings</h3>
<p>I <em>word embeddings</em> catturano informazioni sulla struttura grammaticale. Parole con ruoli sintattici simili si trovano vicine nello spazio vettoriale, riflettendo la <em>coerenza sintattica</em>. Ad esempio, la relazione tra “veloce” e “velocemente” è simile a quella tra “lento” e “lentamente”, esprimibile come: <code>vector("velocemente") - vector("veloce") + vector("lento") ≈ vector("lentamente")</code>. Questa capacità emerge dalle co-occorrenze nei corpora di testo ed è utile per il <em>part-of-speech tagging</em> e il <em>parsing</em> sintattico.</p>
<p>I <em>word embeddings</em> codificano anche relazioni morfologiche: “cane” e “cani” o “camminare” e “camminato” sono vicini, riflettendo una comprensione implicita della morfologia. Modelli come <em>Word2Vec</em> e <em>GloVe</em> non codificano direttamente l’ordine delle parole, ma lo catturano indirettamente tramite co-occorrenze locali. I <em>Transformer</em> (es. <em>BERT</em>) invece considerano l’intero contesto <span class="citation" data-cites="devlin2019">(<a href="#ref-devlin2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>.</p>
<p>Un’altra proprietà è la <em>similitudine funzionale</em>: parole sostituibili in una frase, come “cane” e “gatto” in “Il <em>cane</em> abbaia” e “Il <em>gatto</em> miagola”, hanno vettori simili, utile per l’analisi sintattica e l’estrazione di relazioni.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 47%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Analogia</th>
<th style="text-align: left;">Operazione Vettoriale</th>
<th style="text-align: left;">Risultato Atteso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Veloce : Velocemente :: Lento :?</td>
<td style="text-align: left;"><code>vector("velocemente") - vector("veloce") + vector("lento")</code></td>
<td style="text-align: left;"><code>vector("lentamente")</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">Re : Regno :: Regina :</td>
<td style="text-align: left;"><code>vector("regno") - vector("re") + vector("regina")</code></td>
<td style="text-align: left;"><code>vector("reginato")</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Camminare : Camminato :: Correre :?</td>
<td style="text-align: left;"><code>vector("camminato") - vector("camminare") + vector("correre")</code></td>
<td style="text-align: left;"><code>vector("corso")</code></td>
</tr>
</tbody>
</table>
</section>
<section id="proprietà-semantiche-dei-word-embeddings" class="level3" data-number="15.5.2">
<h3 data-number="15.5.2" class="anchored" data-anchor-id="proprietà-semantiche-dei-word-embeddings"><span class="header-section-number">15.5.2</span> Proprietà Semantiche dei Word Embeddings</h3>
<p>I <em>word embeddings</em> eccellono nel catturare il significato delle parole, posizionando termini simili (es. “cane” e “gatto”) vicini nello spazio vettoriale. La <em>similarità semantica</em> è misurabile con la similarità cosinusoidale: valori alti indicano significati affini. Ad esempio, “automobile” e “veicolo” sono più vicini di “cane” e “gatto”, riflettendo gradi di relazione.</p>
<p>Codificano anche relazioni <em>is-a</em> (iponimia/iperonimia): “automobile” è vicino a “veicolo”, suo iperonimo. L’analogia “re” - “uomo” + “donna” ≈ “regina” dimostra come catturino relazioni semantiche complesse. Gli antonimi (es. “caldo” e “freddo”) sono distanti, ma possono condividere un campo semantico (es. temperatura). Inoltre, raggruppano parole per temi: “ospedale”, “chirurgo” e “infermiere” formano un cluster sanitario.</p>
</section>
<section id="tecniche-di-generazione" class="level3" data-number="15.5.3">
<h3 data-number="15.5.3" class="anchored" data-anchor-id="tecniche-di-generazione"><span class="header-section-number">15.5.3</span> Tecniche di Generazione</h3>
<ul>
<li><em>Word2Vec</em>: Usa reti neurali per apprendere vettori, con <em>Skip-Gram</em> (predice il contesto da una parola) e <em>CBOW</em> (predice la parola dal contesto), efficace per analogie semantiche <span class="citation" data-cites="mikolov2013wordrepresentations">(<a href="#ref-mikolov2013wordrepresentations" role="doc-biblioref">Mikolov et al. 2013</a>)</span>.</li>
<li><em>GloVe</em>: Basato su una matrice di co-occorrenza globale, bilancia relazioni locali e globali <span class="citation" data-cites="pennington2014">(<a href="#ref-pennington2014" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span>.</li>
<li><em>FastText</em>: Estende <em>Word2Vec</em> rappresentando parole come n-grammi di caratteri, ideale per parole rare e lingue morfologicamente ricche <span class="citation" data-cites="bojanowski2017">(<a href="#ref-bojanowski2017" role="doc-biblioref">Bojanowski et al. 2017</a>)</span>.</li>
<li><em>ELMo</em>: Genera embedding contestuali, analizzando l’intera frase per cogliere sfumature <span class="citation" data-cites="peters2018">(<a href="#ref-peters2018" role="doc-biblioref">Peters et al. 2018</a>)</span>.</li>
<li><em>Transformer</em> (es. <em>BERT</em>, <em>RoBERTa</em>): Usa l’attenzione per modellare dipendenze a lunga distanza, migliorando la comprensione sintattica e semantica <span class="citation" data-cites="devlin2019 liu2019">(<a href="#ref-devlin2019" role="doc-biblioref">Devlin et al. 2019</a>; <a href="#ref-liu2019" role="doc-biblioref">Y. Liu et al. 2019</a>)</span>.</li>
</ul>
</section>
</section>
<section id="sentence-embeddings" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="sentence-embeddings"><span class="header-section-number">15.6</span> Sentence Embeddings</h2>
<p>I <em>sentence embeddings</em> rappresentano il significato di intere frasi in vettori, posizionando frasi simili vicine nello spazio vettoriale. Superano i <em>word embeddings</em>, catturando il senso di un pensiero completo, e migliorano attività come classificazione, traduzione e generazione di testo <span class="citation" data-cites="reimers2019">(<a href="#ref-reimers2019" role="doc-biblioref">Reimers and Gurevych 2019</a>)</span>.</p>
<section id="proprietà-dei-sentence-embeddings" class="level3" data-number="15.6.1">
<h3 data-number="15.6.1" class="anchored" data-anchor-id="proprietà-dei-sentence-embeddings"><span class="header-section-number">15.6.1</span> Proprietà dei Sentence Embeddings</h3>
<p>I <em>sentence embeddings</em> codificano il significato complessivo della frase, superando i limiti dei singoli termini. Sono vettori densi in spazi multidimensionali, con modelli come <em>BERT</em> che catturano il contesto di ogni parola. Preservano struttura sintattica e semantica, mappando frasi di lunghezza variabile in vettori fissi per l’apprendimento automatico. Alcuni, multilingue, codificano testi di lingue diverse in uno spazio condiviso, utili per applicazioni cross-linguali.</p>
</section>
<section id="applicazioni" class="level3" data-number="15.6.2">
<h3 data-number="15.6.2" class="anchored" data-anchor-id="applicazioni"><span class="header-section-number">15.6.2</span> Applicazioni</h3>
<ul>
<li><em>Ricerca semantica e recupero</em>: Cerca in base al significato, es. “migliori ristoranti” restituisce “locali di alta cucina”.</li>
<li><em>Classificazione del testo</em>: Classifica articoli (es. sport, politica) in base al significato complessivo.</li>
<li><em>Analisi del sentiment</em>: Determina se una recensione è positiva o negativa, considerando l’intera frase.</li>
<li><em>Rilevamento di parafrasi</em>: Identifica frasi con lo stesso significato, es. domande duplicate in forum.</li>
<li><em>Question answering</em>: Trova risposte pertinenti confrontando similarità semantiche.</li>
<li><em>Traduzione automatica</em>: Migliora la fluidità, cogliendo il contesto della frase.</li>
<li><em>Clustering e modellazione di argomenti</em>: Raggruppa documenti simili per tema.</li>
<li><em>Retrieval-Augmented Generation (RAG)</em>: Recupera passaggi rilevanti per risposte coerenti.</li>
<li><em>Generazione di testo</em>: Produce testo contestualmente appropriato.</li>
</ul>
</section>
<section id="tecniche-per-la-generazione-di-sentence-embeddings" class="level3" data-number="15.6.3">
<h3 data-number="15.6.3" class="anchored" data-anchor-id="tecniche-per-la-generazione-di-sentence-embeddings"><span class="header-section-number">15.6.3</span> Tecniche per la Generazione di Sentence Embeddings</h3>
<ul>
<li><em>Media dei word embeddings</em>: Metodo base, ma perde ordine e relazioni complesse.</li>
<li><em>Modelli dedicati</em>: <em>Sentence-BERT</em>, <em>Universal Sentence Encoder</em> e <em>InferSent</em> generano embedding di alta qualità <span class="citation" data-cites="reimers2019 cer2018">(<a href="#ref-reimers2019" role="doc-biblioref">Reimers and Gurevych 2019</a>; <a href="#ref-cer2018" role="doc-biblioref">Cer et al. 2018</a>)</span>.</li>
<li><em>Transformer</em>: <em>BERT</em>, <em>RoBERTa</em> e altri usano pooling (es. media, token CLS) per sfruttare il contesto <span class="citation" data-cites="devlin2019">(<a href="#ref-devlin2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>.</li>
</ul>
</section>
</section>
<section id="transformer" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="transformer"><span class="header-section-number">15.7</span> Transformer</h2>
<p>La comprensione e generazione del linguaggio naturale sono essenziali in ambito legale, per analizzare contratti o sentenze. Le reti neurali ricorrenti (RNN), come le <em>LSTM</em>, perdono informazioni su testi lunghi. I <em>Transformer</em>, introdotti nel 2017, rivoluzionano l’NLP con il meccanismo di <em>attenzione</em> <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>.</p>
<section id="il-cuore-dei-transformer-lattenzione" class="level3" data-number="15.7.1">
<h3 data-number="15.7.1" class="anchored" data-anchor-id="il-cuore-dei-transformer-lattenzione"><span class="header-section-number">15.7.1</span> Il Cuore dei Transformer: l’attenzione</h3>
<p>L’<strong>attenzione</strong> si concentra sulle parti rilevanti di un testo. In una clausola come “Il locatore, dopo aver ricevuto il pagamento del canone mensile entro il 5 del mese, garantisce al conduttore l’uso esclusivo dei locali commerciali situati in Via Roma 10, salvo eventuali violazioni delle norme di sicurezza specificate nell’Allegato B”, il modello collega “conduttore” a “locatore” e “Allegato B”, dando meno peso a “mensile” o “5”.</p>
<p>L’<strong>attenzione</strong> si calcola con:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<ul>
<li><em>Q (Query)</em>: rappresenta la parola analizzata (es. “conduttore”).</li>
<li><em>K (Key)</em>: valuta la rilevanza di altre parole.</li>
<li><em>V (Value)</em>: aggrega le informazioni ponderate.</li>
<li><em>√d_k</em>: normalizza i punteggi per stabilità.</li>
<li><em>softmax</em>: trasforma i punteggi in pesi.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Parola (Key)</th>
<th>Peso di Attenzione (ipotetico)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>locatore</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>locali commerciali</td>
<td>0.20</td>
</tr>
<tr class="odd">
<td>Allegato B</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>al</td>
<td>0.10</td>
</tr>
<tr class="odd">
<td>canone</td>
<td>0.05</td>
</tr>
<tr class="even">
<td>mensile</td>
<td>0.02</td>
</tr>
<tr class="odd">
<td>dopo</td>
<td>0.01</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>Questo collega parole distanti, cruciale per contratti o pareri legali.</p>
</section>
<section id="multi-head-attention-vedere-il-linguaggio-da-diverse-prospettive" class="level3" data-number="15.7.2">
<h3 data-number="15.7.2" class="anchored" data-anchor-id="multi-head-attention-vedere-il-linguaggio-da-diverse-prospettive"><span class="header-section-number">15.7.2</span> Multi-Head Attention: Vedere il Linguaggio da Diverse Prospettive</h3>
<p>La <em>multi-head attention</em> esegue l’<strong>attenzione</strong> in parallelo: una testa analizza la sintassi, un’altra la semantica, per una comprensione più completa.</p>
</section>
<section id="larchitettura-del-transformer-encoder-e-decoder" class="level3" data-number="15.7.3">
<h3 data-number="15.7.3" class="anchored" data-anchor-id="larchitettura-del-transformer-encoder-e-decoder"><span class="header-section-number">15.7.3</span> L’Architettura del Transformer: Encoder e Decoder</h3>
<ul>
<li><strong>Encoder</strong>: Trasforma il testo (es. un contratto) in vettori ricchi di contesto, con strati di <em>multi-head attention</em> e reti feed-forward.</li>
<li><strong>Decoder</strong>: Usa l’output dell’<strong>encoder</strong> e le parole generate per produrre testi, come riassunti o risposte.</li>
</ul>
</section>
<section id="perché-i-transformer-hanno-rivoluzionato-lnlp" class="level3" data-number="15.7.4">
<h3 data-number="15.7.4" class="anchored" data-anchor-id="perché-i-transformer-hanno-rivoluzionato-lnlp"><span class="header-section-number">15.7.4</span> Perché i Transformer Hanno Rivoluzionato l’NLP?</h3>
<ul>
<li><strong>Parallelizzazione</strong>: Elabora tutte le parole simultaneamente, più veloce delle RNN.</li>
<li><strong>Dipendenze a lunga distanza</strong>: Collega informazioni distanti, es. clausole e allegati.</li>
<li><strong>Scalabilità</strong>: Modelli come <em>BERT</em> e <em>GPT</em> eccellono su grandi dataset, utili per:
<ul>
<li>Classificazione di documenti legali.</li>
<li>Ricerca giuridica.</li>
<li>Generazione e traduzione di testi.</li>
</ul></li>
</ul>
</section>
</section>
<section id="large-language-models-llm" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="large-language-models-llm"><span class="header-section-number">15.8</span> Large Language Models (LLM)</h2>
<p>Un <em>LLM</em> è un modello di machine learning addestrato su vasti corpora di testo per comprendere e generare linguaggio, utile per classificazione, ricerca, generazione e traduzione <span class="citation" data-cites="brown2020">(<a href="#ref-brown2020" role="doc-biblioref">Brown et al. 2020</a>)</span>. Modelli noti includono:</p>
<ol type="1">
<li><em>GPT-4 (OpenAI)</em>: Denso, multimodale, contesto fino a 128K token.</li>
<li><em>Grok (xAI)</em>: Progettato per risposte rapide e analisi di dati social.</li>
<li><em>Claude (Anthropic)</em>: Sicuro, con contesto fino a 200K token.</li>
<li><em>Gemini (Google DeepMind)</em>: Multimodale, contesto fino a 1M token.</li>
<li><em>DeepSeek (DeepSeek AI)</em>: <em>MoE</em>, efficiente, 671B parametri.</li>
<li><em>Qwen (Alibaba)</em>: <em>MoE</em>, multilingue, 128K token.</li>
</ol>
<p>Sono accessibili come chatbot (per prompt testuali) o API (per richieste HTTP). I costi variano in base al provider, al modello, al volume di token e alla regione. Per dettagli, consultare i siti ufficiali (es. <a href="https://x.ai/grok" class="uri">https://x.ai/grok</a> per Grok, <a href="https://openai.com" class="uri">https://openai.com</a> per GPT).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Mixture of Experts (MoE)</strong> I modelli <em>Mixture of Experts (MoE)</em> dividono un problema complesso in sottoproblemi, assegnandoli a “esperti” specializzati (sottoreti neurali). Una rete di <em>gating</em> sceglie l’esperto adatto per l’input. L’<em>attivazione sparsa</em> coinvolge solo alcuni esperti, riducendo il carico computazionale pur mantenendo molti parametri <span class="citation" data-cites="shazeer2017">(<a href="#ref-shazeer2017" role="doc-biblioref">Shazeer et al. 2017</a>)</span>.</p>
</div>
</div>
<section id="prompt-engineering" class="level3" data-number="15.8.1">
<h3 data-number="15.8.1" class="anchored" data-anchor-id="prompt-engineering"><span class="header-section-number">15.8.1</span> Prompt Engineering</h3>
<p>La <em>prompt engineering</em> ottimizza i prompt per ottenere risposte precise e coerenti dagli <em>LLM</em>, strutturando istruzioni, contesti ed esempi per massimizzare rilevanza e accuratezza <span class="citation" data-cites="liu2023">(<a href="#ref-liu2023" role="doc-biblioref">P. Liu et al. 2023</a>)</span>.</p>
<section id="strategie-di-prompt-engineering" class="level4" data-number="15.8.1.1">
<h4 data-number="15.8.1.1" class="anchored" data-anchor-id="strategie-di-prompt-engineering"><span class="header-section-number">15.8.1.1</span> Strategie di Prompt Engineering</h4>
<ul>
<li><em>Specificità del prompt</em>: Istruzioni chiare evitano ambiguità.<br>
<em>Esempio</em>: “Riassumi la sentenza Cass. Civ., Sez. Unite, n.&nbsp;500/1999, evidenziando i punti salienti relativi all’articolo 2043 del Codice Civile in materia di danno ingiusto, e indica la ratio decidendi in non più di 200 parole.”</li>
<li><em>Few-shot prompting</em>: Fornisce esempi di input/output.<br>
<em>Esempio</em>: “Esempio 1: Input: ‘Clausola di riservatezza per contratto di lavoro.’ Output: ‘Il dipendente si impegna a mantenere riservate le informazioni aziendali, pena il risarcimento del danno.’ Ora, scrivi una clausola per un contratto di locazione che preveda un indennizzo per mancato preavviso di recesso.”</li>
<li><em>Chain-of-thought prompting</em>: Esplicita i passaggi logici.<br>
<em>Esempio</em>: “Considera un caso di responsabilità contrattuale: [descrizione]. Pensa: 1. Quali sono gli elementi della responsabilità? 2. Le prove dimostrano l’inadempimento? 3. C’è un nesso di causalità? 4. Quali sono le conseguenze giuridiche? Concludi.”</li>
<li><em>Impersonificazione di un ruolo</em>: Simula un professionista.<br>
<em>Esempio</em>: “Agisci come un giudice della Corte Costituzionale. Esamina la legittimità costituzionale dell’articolo X della legge Y, con un parere formale e motivato.”</li>
</ul>
</section>
</section>
<section id="modelli-llm-locali" class="level3" data-number="15.8.2">
<h3 data-number="15.8.2" class="anchored" data-anchor-id="modelli-llm-locali"><span class="header-section-number">15.8.2</span> Modelli LLM “Locali”</h3>
<p>L’uso di <em>LLM</em> via API di provider come OpenAI o Google è costoso e pone problemi di privacy. I modelli <em>locali</em> si eseguono su CPU e GPU standard, usando tecniche come:</p>
<ol type="1">
<li><em>Quantizzazione</em>: Riduce i pesi a 8, 4 o 2 bit (<em>GPTQ</em>, <em>bitsandbytes</em>) <span class="citation" data-cites="dettmers2023">(<a href="#ref-dettmers2023" role="doc-biblioref">Dettmers et al. 2023</a>)</span>.</li>
<li><em>Distillazione</em>: Un modello piccolo imita uno grande (<em>DistilBERT</em>) <span class="citation" data-cites="sanh2019">(<a href="#ref-sanh2019" role="doc-biblioref">Sanh et al. 2019</a>)</span>.</li>
<li><em>Pruning</em>: Elimina pesi poco rilevanti.</li>
<li><em>Low-Rank Adaptation (LoRA)</em>: Aggiunge matrici addestrabili ai pesi congelati <span class="citation" data-cites="hu2021">(<a href="#ref-hu2021" role="doc-biblioref">Hu et al. 2021</a>)</span>.</li>
<li><em>MoE</em>: Attiva solo sotto-reti rilevanti <span class="citation" data-cites="shazeer2017">(<a href="#ref-shazeer2017" role="doc-biblioref">Shazeer et al. 2017</a>)</span>.</li>
<li><em>Ottimizzazione hardware</em>: Usa <em>ONNX</em>, <em>TensorRT</em>, <em>GGUF</em>.</li>
</ol>
<p>Piattaforme open source:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 48%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Piattaforma</strong></th>
<th><strong>Descrizione</strong></th>
<th><strong>Supporto modelli</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>llama.cpp</em></td>
<td>Esecuzione di <em>LLaMA</em> su CPU, anche mobile.</td>
<td><em>GGUF</em> (<em>LLaMA</em>, <em>Mistral</em>)</td>
</tr>
<tr class="even">
<td><em>MLC LLM</em></td>
<td>Compilatore per WebGPU, iOS, Android.</td>
<td><em>LLaMA</em>, <em>Mistral</em>, <em>Qwen</em></td>
</tr>
<tr class="odd">
<td><em>Ollama</em></td>
<td>Esecuzione semplice con un comando.</td>
<td><em>LLaMA</em>, <em>Mistral</em>, <em>Gemma</em></td>
</tr>
</tbody>
</table>
<p>Piattaforme proprietarie:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 48%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Piattaforma</strong></th>
<th><strong>Descrizione</strong></th>
<th><strong>Note</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>OpenVINO™</em></td>
<td>Ottimizza modelli su CPU e VPU (Intel).</td>
<td>Supporta <em>ONNX</em></td>
</tr>
<tr class="even">
<td><em>NVIDIA TensorRT-LLM</em></td>
<td>Ottimizza <em>LLM</em> su GPU NVIDIA.</td>
<td>Uso enterprise</td>
</tr>
</tbody>
</table>
<p><em>Nota</em>: <em>FlashAttention</em> è una tecnica per ottimizzare l’attenzione, riducendo il consumo di memoria <span class="citation" data-cites="dao2022">(<a href="#ref-dao2022" role="doc-biblioref">Dao et al. 2022</a>)</span>.</p>
</section>
</section>
<section id="applicazioni-in-giurisprudenza" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="applicazioni-in-giurisprudenza"><span class="header-section-number">15.9</span> Applicazioni in Giurisprudenza</h2>
<p>L’NLP rivoluziona il settore legale <span class="citation" data-cites="surden2019 ashley2017">(<a href="#ref-surden2019" role="doc-biblioref">Surden 2019</a>; <a href="#ref-ashley2017" role="doc-biblioref">Ashley 2017</a>)</span>:</p>
<ul>
<li><em>Analisi legale</em>:
<ul>
<li>Revisiona contratti per clausole o ambiguità.<br>
</li>
<li>Cerca precedenti con <em>NER</em> e <em>topic modeling</em>.<br>
</li>
<li>Interpreta leggi con analisi semantica.</li>
</ul></li>
<li><em>Assistenza giuridica</em>:
<ul>
<li>Chatbot per consulenza preliminare.<br>
</li>
<li>Automatizza risposte e gestione clienti.</li>
</ul></li>
<li><em>Previsione di esiti</em>: Analizza sentenze per prevedere risultati.</li>
<li><em>Gestione documentale</em>:
<ul>
<li>Classifica e archivia documenti.<br>
</li>
<li>Estrae dati chiave per database.</li>
</ul></li>
<li><em>Supporto alla decisione</em>: Aiuta i giudici con citazioni e precedenti.</li>
<li><em>Compliance e due diligence</em>: Monitora conformità e rischi.</li>
<li><em>Innovazione</em>: Crea modelli di documenti e simula negoziazioni.</li>
<li><em>Etica e accessibilità</em>: Traduce testi e semplifica il linguaggio legale.</li>
</ul>
</section>
<section id="sfide-e-considerazioni-etiche" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="sfide-e-considerazioni-etiche"><span class="header-section-number">15.10</span> Sfide e Considerazioni Etiche</h2>
<ul>
<li><em>Bias e neutralità</em>: Gli <em>LLM</em> possono amplificare pregiudizi; servono dati diversificati e de-biasing <span class="citation" data-cites="bender2021">(<a href="#ref-bender2021" role="doc-biblioref">Bender et al. 2021</a>)</span>.</li>
<li><em>Privacy</em>: Rispettare il GDPR, anonimizzando i dati.</li>
<li><em>Responsabilità e trasparenza</em>: Chiarire chi è responsabile per errori e rendere i modelli spiegabili.</li>
<li><em>Accuratezza</em>: Verifica umana per evitare errori gravi.</li>
<li><em>Equità nell’accesso</em>: Evitare divari tra grandi e piccoli studi legali.</li>
<li><em>Manipolazione</em>: Rilevare falsi documenti generati.</li>
<li><em>Etica</em>: Bilanciare automazione e impatto sul lavoro.</li>
<li><em>Regolamentazione</em>: Collaborare per norme sull’NLP.</li>
</ul>
</section>
<section id="conclusioni" class="level2" data-number="15.11">
<h2 data-number="15.11" class="anchored" data-anchor-id="conclusioni"><span class="header-section-number">15.11</span> Conclusioni</h2>
<p>L’NLP offre strumenti potenti per la giurisprudenza, ma richiede attenzione a etica e implicazioni legali. Questo capitolo introduce l’NLP e le sue applicazioni, preparando a ulteriori ricerche.</p>
</section>
<section id="laboratorio-di-python" class="level2" data-number="15.12">
<h2 data-number="15.12" class="anchored" data-anchor-id="laboratorio-di-python"><span class="header-section-number">15.12</span> Laboratorio di Python</h2>
<section id="esperimento-1-introduzione-ai-word-embeddings" class="level3" data-number="15.12.1">
<h3 data-number="15.12.1" class="anchored" data-anchor-id="esperimento-1-introduzione-ai-word-embeddings"><span class="header-section-number">15.12.1</span> Esperimento 1: Introduzione ai Word Embeddings</h3>
<p>In questo esperimento vogliamo implementare i word embedding in maniera semplificata e intuitiva per cercare di capire le notevoli proprietà che riescono a esprimere. I <em>word embeddings</em> sono vettori numerici. Qui li rappresentiamo in 3D con assi per <em>regalità</em>, <em>genere</em> ed <em>età</em> per parole come “re”, “regina”, ecc.</p>
<div id="65ee19df" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Word embeddings semplificati</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>word_embeddings <span class="op">=</span> {</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"re"</span>: [<span class="fl">5.0</span>, <span class="fl">5.0</span>, <span class="fl">5.0</span>],</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"regina"</span>: [<span class="fl">5.0</span>, <span class="op">-</span><span class="fl">5.0</span>, <span class="fl">5.0</span>],</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"principe"</span>: [<span class="fl">3.0</span>, <span class="fl">5.0</span>, <span class="fl">2.0</span>],</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"principessa"</span>: [<span class="fl">3.0</span>, <span class="op">-</span><span class="fl">5.0</span>, <span class="fl">2.0</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"uomo"</span>: [<span class="fl">1.0</span>, <span class="fl">5.0</span>, <span class="fl">5.0</span>],</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"donna"</span>: [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">5.0</span>, <span class="fl">5.0</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">list</span>(word_embeddings.keys())</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> <span class="bu">list</span>(word_embeddings.values())</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Grafico 3D</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [emb[<span class="dv">0</span>] <span class="cf">for</span> emb <span class="kw">in</span> embeddings]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [emb[<span class="dv">1</span>] <span class="cf">for</span> emb <span class="kw">in</span> embeddings]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> [emb[<span class="dv">2</span>] <span class="cf">for</span> emb <span class="kw">in</span> embeddings]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, y, z, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    ax.text(embeddings[i][<span class="dv">0</span>], embeddings[i][<span class="dv">1</span>], embeddings[i][<span class="dv">2</span>], word)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Regalità'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Genere'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">'Età'</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Visualizzazione 3D dei Word Embeddings'</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax.view_init(elev<span class="op">=</span><span class="dv">20</span>, azim<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Distanza coseno</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distanza_coseno(vec1, vec2):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    prodotto_scalare <span class="op">=</span> np.dot(vec1, vec2)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    norma_vec1 <span class="op">=</span> np.linalg.norm(vec1)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    norma_vec2 <span class="op">=</span> np.linalg.norm(vec2)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">-</span> (prodotto_scalare <span class="op">/</span> (norma_vec1 <span class="op">*</span> norma_vec2))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrice delle distanze</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>n_words <span class="op">=</span> <span class="bu">len</span>(words)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>matrice_distanze <span class="op">=</span> [[<span class="fl">0.0</span>] <span class="op">*</span> n_words <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_words)]</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_words):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_words):</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        matrice_distanze[i][j] <span class="op">=</span> distanza_coseno(embeddings[i], embeddings[j])</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizzazione</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrice delle distanze coseno:"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"          "</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">:&gt;10}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word1 <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word1<span class="sc">:10}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_words):</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>matrice_distanze[i][j]<span class="sc">:10.3f}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcolo di "regina"</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sottrai_vec(vec1, vec2):</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(vec1) <span class="op">-</span> np.array(vec2)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> somma_vec(vec1, vec2):</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(vec1) <span class="op">+</span> np.array(vec2)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Word embedding di regina:"</span>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(word_embeddings[<span class="st">"regina"</span>])</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Word embedding calcolato: regina = re - uomo + donna"</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(somma_vec(sottrai_vec(word_embeddings[<span class="st">"re"</span>], word_embeddings[<span class="st">"uomo"</span>]), word_embeddings[<span class="st">"donna"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-7-elaborazione-linguaggio-naturale_files/figure-html/cell-2-output-1.png" width="611" height="631" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Matrice delle distanze coseno:
                  re    regina  principeprincipessa      uomo     donna
re             0.000     0.667     0.063     1.000     0.111     0.919
regina         0.667     0.000     1.000     0.063     0.919     0.111
principe       0.063     1.000    -0.000     1.316     0.137     1.273
principessa     1.000     0.063     1.316    -0.000     1.273     0.137
uomo           0.111     0.919     0.137     1.273     0.000     0.980
donna          0.919     0.111     1.273     0.137     0.980     0.000
Word embedding di regina:
[5.0, -5.0, 5.0]
Word embedding calcolato: regina = re - uomo + donna
[ 5. -5.  5.]</code></pre>
</div>
</div>
<p>I risulrari numerici dell’esperimento ci mostrano che anche se in una versione molto semplificata, i word embedding sono in grado di esprimere proprietà come la regalità, il genere e l’età. Infatti si noti come sottraendo a <strong>re</strong> <strong>uomo</strong> si ottiene la regalità che può essere sommata a <strong>donna</strong> per ottenere <strong>regina</strong>.</p>
</section>
<section id="esperimento-2-analisi-del-sentiment" class="level3" data-number="15.12.2">
<h3 data-number="15.12.2" class="anchored" data-anchor-id="esperimento-2-analisi-del-sentiment"><span class="header-section-number">15.12.2</span> Esperimento 2: Analisi del Sentiment</h3>
<p>In questo esperimento vogliamo stimare il sentiment (molto negativo, negativo, neutrale, positivo, molto positivo) di un frammento di testo. A questo scopo usiamo un modello addestrato su più lingue per stimare il sentiment in frammenti di testo in Italiano e Giapponese.Il modello prescelto è tabularisai/multilingual-sentiment-analysis di Hugging Face.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Nota: Richiede pip install transformers e una connessione per scaricare il modello.</p>
</div>
</div>
<div id="4083d532" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline con modello multilingue</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"tabularisai/multilingual-sentiment-analysis"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Testo in Italiano</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>frase <span class="op">=</span> <span class="st">"Questo prodotto non è fatto bene."</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>risultato <span class="op">=</span> pipe(frase)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(frase)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(risultato)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Testo tradotto in Giapponese </span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>frase <span class="op">=</span> <span class="st">"この製品はよく作られていません。"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>risultato <span class="op">=</span> pipe(frase)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(frase)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(risultato)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING:tensorflow:From C:\Users\lcapitanio\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

Questo prodotto non è fatto bene.
[{'label': 'Negative', 'score': 0.5313267111778259}]
この製品はよく作られていません。
[{'label': 'Negative', 'score': 0.48841869831085205}]</code></pre>
</div>
</div>
<p>L’output dell’esperimento ci dice che entrambre le frasi sono considerate negative. Si noti come la frase in Giapponese sia la traduzione della frase in Italiano fatta da un traduttore basato su LLM e non mostrato in questo esempio.</p>
</section>
<section id="esperimento-3-sistema-di-domanda-risposta-su-un-testo-giuridico" class="level3" data-number="15.12.3">
<h3 data-number="15.12.3" class="anchored" data-anchor-id="esperimento-3-sistema-di-domanda-risposta-su-un-testo-giuridico"><span class="header-section-number">15.12.3</span> Esperimento 3: Sistema di Domanda-Risposta su un Testo Giuridico</h3>
<p>In questo esperimento vogliamo implementare un semplicissimo sistema che ci consenta di di dialogare con un breve testo giuridico. Facendo domande e ottenendo risposte sui contenuti del testo.Il modello adottato è “deepset/roberta-base-squad2” sempre da Hugging Faceper rispondere a domande su un testo giuridico.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Nota: Richiede pip install transformers e una connessione per scaricare il modello.</p>
</div>
</div>
<div id="64b4b50e" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pipeline per question answering</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>qa_pipeline <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>, model<span class="op">=</span><span class="st">"deepset/roberta-base-squad2"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Contesto giuridico</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>contesto <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="st">L'articolo 3 della Costituzione italiana stabilisce che tutti i cittadini hanno pari dignità sociale e sono eguali davanti alla legge,</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="st">senza distinzione di sesso, razza, lingua, religione, opinioni politiche, condizioni personali e sociali.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="st">È compito della Repubblica rimuovere gli ostacoli di ordine economico e sociale che,</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="st">limitando di fatto la libertà e l'eguaglianza dei cittadini, impediscono il pieno sviluppo della persona umana.</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Domande</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>domanda1 <span class="op">=</span> <span class="st">"Qual è il compito della Repubblica?"</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>risultato1 <span class="op">=</span> qa_pipeline(question<span class="op">=</span>domanda1, context<span class="op">=</span>contesto)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Domanda: </span><span class="sc">{</span>domanda1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Risposta: </span><span class="sc">{</span>risultato1[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>domanda2 <span class="op">=</span> <span class="st">"Chi ha pari dignità sociale?"</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>risultato2 <span class="op">=</span> qa_pipeline(question<span class="op">=</span>domanda2, context<span class="op">=</span>contesto)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Domanda: </span><span class="sc">{</span>domanda2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Risposta: </span><span class="sc">{</span>risultato2[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>domanda3 <span class="op">=</span> <span class="st">"Quale articolo della Costituzione stabilisce che tutti i cittadini hanno pari dignità sociale?"</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>risultato3 <span class="op">=</span> qa_pipeline(question<span class="op">=</span>domanda3, context<span class="op">=</span>contesto)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Domanda: </span><span class="sc">{</span>domanda3<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Risposta: </span><span class="sc">{</span>risultato3[<span class="st">'answer'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Domanda: Qual è il compito della Repubblica?
Risposta: rimuovere gli ostacoli

Domanda: Chi ha pari dignità sociale?
Risposta: tutti i cittadini

Domanda: Quale articolo della Costituzione stabilisce che tutti i cittadini hanno pari dignità sociale?
Risposta: L'articolo 3
</code></pre>
</div>
</div>
<p>Dall’output dell’esperimento possiamo vedere che il sistema è in grado di rispondere a domande sul testo giuridico. Ovviamente il sistema non è in grado di rispondere a domande il cui oggetto non è specificato nel testo. Inoltre, la brevità del testo usata non consente di rispondere a domande che richiedono un’analisi più approfondita.</p>
</section>
</section>
<section id="esercizi" class="level2" data-number="15.13">
<h2 data-number="15.13" class="anchored" data-anchor-id="esercizi"><span class="header-section-number">15.13</span> Esercizi</h2>
<section id="esercizio-1-pre-elaborazione-di-un-testo-giuridico" class="level3" data-number="15.13.1">
<h3 data-number="15.13.1" class="anchored" data-anchor-id="esercizio-1-pre-elaborazione-di-un-testo-giuridico"><span class="header-section-number">15.13.1</span> Esercizio 1: Pre-elaborazione di un Testo Giuridico</h3>
<p>Scegli un breve articolo di legge o una clausola contrattuale (es. dall’articolo 3 della Costituzione italiana o da un contratto di locazione). Scrivi un programma Python che applichi le seguenti operazioni di pre-elaborazione: tokenizzazione, rimozione delle stopword (usando una lista predefinita, es. da nltk), conversione in minuscolo e lemmatizzazione (con spacy o nltk). Visualizza il testo originale e quello pre-elaborato. Rifletti: quali operazioni sono più utili per preparare il testo a un’analisi semantica in ambito legale?</p>
</section>
<section id="esercizio-2-creazione-di-un-prompt-efficace" class="level3" data-number="15.13.2">
<h3 data-number="15.13.2" class="anchored" data-anchor-id="esercizio-2-creazione-di-un-prompt-efficace"><span class="header-section-number">15.13.2</span> Esercizio 2: Creazione di un prompt efficace</h3>
<p>Progetta un prompt per un Large Language Model (es. usando few-shot prompting o chain-of-thought) per generare un riassunto di una sentenza giuridica (es. Cass. Civ., Sez. Unite, n.&nbsp;500/1999). Il prompt deve specificare: lunghezza massima (es. 150 parole), elementi chiave da includere (es. fatti, decisione, ratio decidendi) e tono formale. Testa il prompt con un modello accessibile (es. tramite Hugging Face o un’API gratuita) e valuta la qualità del riassunto prodotto. Quali modifiche al prompt migliorerebbero la precisione e la chiarezza del risultato?</p>
</section>
<section id="esercizio-3-analisi-semantica-con-word-embeddings" class="level3" data-number="15.13.3">
<h3 data-number="15.13.3" class="anchored" data-anchor-id="esercizio-3-analisi-semantica-con-word-embeddings"><span class="header-section-number">15.13.3</span> Esercizio 3: Analisi semantica con word embeddings</h3>
<p>Modifica il codice dell’Esempio 1 del laboratorio Python per aggiungere nuove parole legate al contesto giuridico (es. “giudice”, “avvocato”, “tribunale”, “sentenza”). Assegna loro vettori 3D basati su tre dimensioni rilevanti (es. autorità, ruolo, formalità). Calcola la similarità cosinusoidale tra queste parole e visualizza i risultati in una matrice. Estendi il codice per verificare un’analogia giuridica (es. “giudice : tribunale :: avvocato : ?”). Discuti come i word embeddings potrebbero supportare la ricerca di precedenti legali o l’estrazione di concetti da documenti giuridici.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ashley2017" class="csl-entry" role="listitem">
Ashley, Kevin D. 2017. <span>“Artificial Intelligence and Legal Analytics: New Tools for Law Practice in the Digital Age.”</span> <em>Cambridge University Press</em>.
</div>
<div id="ref-bender2021" class="csl-entry" role="listitem">
Bender, Emily M, Timnit Gebru, Angelina McMillan-Major, and Margaret Shmitchell. 2021. <span>“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”</span> <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 610–23.
</div>
<div id="ref-bojanowski2017" class="csl-entry" role="listitem">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. <span>“Enriching Word Vectors with Subword Information.”</span> <em>Transactions of the Association for Computational Linguistics</em> 5: 135–46.
</div>
<div id="ref-brown2020" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div>
<div id="ref-cer2018" class="csl-entry" role="listitem">
Cer, Daniel, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, et al. 2018. <span>“Universal Sentence Encoder.”</span> <em>arXiv Preprint arXiv:1803.11175</em>.
</div>
<div id="ref-dao2022" class="csl-entry" role="listitem">
Dao, Tri, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. <span>“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 16344–59.
</div>
<div id="ref-dettmers2023" class="csl-entry" role="listitem">
Dettmers, Tim, Artem Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. <span>“QLoRA: Efficient Finetuning of Quantized LLMs.”</span> <em>arXiv Preprint arXiv:2305.14314</em>.
</div>
<div id="ref-devlin2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86.
</div>
<div id="ref-hu2021" class="csl-entry" role="listitem">
Hu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“LoRA: Low-Rank Adaptation of Large Language Models.”</span> <em>arXiv Preprint arXiv:2106.09685</em>.
</div>
<div id="ref-liu2023" class="csl-entry" role="listitem">
Liu, Pengfei, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. <span>“Prompt Engineering for Large Language Models.”</span> <em>arXiv Preprint arXiv:2307.09067</em>.
</div>
<div id="ref-liu2019" class="csl-entry" role="listitem">
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. <span>“RoBERTa: A Robustly Optimized BERT Pretraining Approach.”</span> <em>arXiv Preprint arXiv:1907.11692</em>.
</div>
<div id="ref-mikolov2013wordrepresentations" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-pennington2014" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. <span>“GloVe: Global Vectors for Word Representation.”</span> <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43.
</div>
<div id="ref-peters2018" class="csl-entry" role="listitem">
Peters, Matthew E, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>“Deep Contextualized Word Representations.”</span> <em>arXiv Preprint arXiv:1802.05365</em>.
</div>
<div id="ref-reimers2019" class="csl-entry" role="listitem">
Reimers, Nils, and Iryna Gurevych. 2019. <span>“Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.”</span> <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 3982–92.
</div>
<div id="ref-sanh2019" class="csl-entry" role="listitem">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. <span>“DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.”</span> <em>arXiv Preprint arXiv:1910.01108</em>.
</div>
<div id="ref-shazeer2017" class="csl-entry" role="listitem">
Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Geoffrey Hinton, Quoc Le, and Jeff Dean. 2017. <span>“Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.”</span> <em>arXiv Preprint arXiv:1701.06538</em>.
</div>
<div id="ref-surden2019" class="csl-entry" role="listitem">
Surden, Harry. 2019. <span>“Artificial Intelligence and Law: An Overview.”</span> <em>Georgia State University Law Review</em> 35 (4): 1305–37.
</div>
<div id="ref-vaswani2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "capitanio/laboratorio-ia";
    script.dataset.repoId = "";
    script.dataset.category = "General";
    script.dataset.categoryId = "";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./3-6-deep-learning.html" class="pagination-link" aria-label="Deep learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Deep learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bibliografia.html" class="pagination-link" aria-label="Bibliografia">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bibliografia</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Laboratorio di Intelligenza Artificiale (1e) scritto da Luciano Capitanio.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Draft - Licenza Apache ver. 2</p>
</div>
  </div>
</footer>




</body></html>