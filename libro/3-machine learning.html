<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VNK2YDR2HZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VNK2YDR2HZ');
</script>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine learning – Laboratorio di Intelligenza Artificiale</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./a1-elementi-di-Python.html" rel="next">
<link href="./2-algoritmi.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-machine learning.html"><span class="chapter-title">Machine learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Laboratorio di Intelligenza Artificiale</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./libro.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./libro.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benvenuti</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0-introduzione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Turing-vs-Searle.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Turing vs Searle: Un’Analisi del Pensiero Computazionale e della Coscienza</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-algoritmi.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Algoritmi</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-machine learning.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Machine learning</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-elementi-di-Python.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Elementi di Python</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#apprendimento-supervisionato" id="toc-apprendimento-supervisionato" class="nav-link active" data-scroll-target="#apprendimento-supervisionato">Apprendimento Supervisionato</a>
  <ul class="collapse">
  <li><a href="#definizione-e-principi-di-base" id="toc-definizione-e-principi-di-base" class="nav-link" data-scroll-target="#definizione-e-principi-di-base">Definizione e Principi di Base</a></li>
  <li><a href="#classificazione" id="toc-classificazione" class="nav-link" data-scroll-target="#classificazione">Classificazione</a></li>
  <li><a href="#regressione" id="toc-regressione" class="nav-link" data-scroll-target="#regressione">Regressione</a></li>
  <li><a href="#algoritmi-principali-dellapprendimento-supervisionato" id="toc-algoritmi-principali-dellapprendimento-supervisionato" class="nav-link" data-scroll-target="#algoritmi-principali-dellapprendimento-supervisionato">Algoritmi Principali dell’Apprendimento Supervisionato</a></li>
  <li><a href="#apprendimento-per-rinforzo" id="toc-apprendimento-per-rinforzo" class="nav-link" data-scroll-target="#apprendimento-per-rinforzo">Apprendimento per Rinforzo</a></li>
  <li><a href="#overfitting-e-underfitting" id="toc-overfitting-e-underfitting" class="nav-link" data-scroll-target="#overfitting-e-underfitting">Overfitting e Underfitting</a></li>
  <li><a href="#valutazione-dei-modelli" id="toc-valutazione-dei-modelli" class="nav-link" data-scroll-target="#valutazione-dei-modelli">Valutazione dei Modelli</a></li>
  </ul></li>
  <li><a href="#apprendimento-non-supervisionato" id="toc-apprendimento-non-supervisionato" class="nav-link" data-scroll-target="#apprendimento-non-supervisionato">Apprendimento Non Supervisionato</a>
  <ul class="collapse">
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering">Clustering</a></li>
  <li><a href="#riduzione-della-dimensionalità" id="toc-riduzione-della-dimensionalità" class="nav-link" data-scroll-target="#riduzione-della-dimensionalità">Riduzione della Dimensionalità</a></li>
  <li><a href="#algoritmi-principali" id="toc-algoritmi-principali" class="nav-link" data-scroll-target="#algoritmi-principali">Algoritmi Principali</a></li>
  <li><a href="#applicazioni-1" id="toc-applicazioni-1" class="nav-link" data-scroll-target="#applicazioni-1">Applicazioni</a></li>
  </ul></li>
  <li><a href="#bias" id="toc-bias" class="nav-link" data-scroll-target="#bias">Bias</a>
  <ul class="collapse">
  <li><a href="#tipologie-di-bias" id="toc-tipologie-di-bias" class="nav-link" data-scroll-target="#tipologie-di-bias">Tipologie di Bias</a></li>
  <li><a href="#cause-e-impatti-del-bias" id="toc-cause-e-impatti-del-bias" class="nav-link" data-scroll-target="#cause-e-impatti-del-bias">Cause e Impatti del Bias</a></li>
  <li><a href="#tecniche-di-mitigazione-del-bias" id="toc-tecniche-di-mitigazione-del-bias" class="nav-link" data-scroll-target="#tecniche-di-mitigazione-del-bias">Tecniche di Mitigazione del Bias</a></li>
  </ul></li>
  <li><a href="#reti-neurali-1" id="toc-reti-neurali-1" class="nav-link" data-scroll-target="#reti-neurali-1">Reti Neurali</a>
  <ul class="collapse">
  <li><a href="#regressione-lineare-e-logistica" id="toc-regressione-lineare-e-logistica" class="nav-link" data-scroll-target="#regressione-lineare-e-logistica">Regressione Lineare e Logistica</a></li>
  <li><a href="#percettrone" id="toc-percettrone" class="nav-link" data-scroll-target="#percettrone">Percettrone</a></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning">Deep learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Machine learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Benvenuti al Capitolo 4 del nostro “Laboratorio di Intelligenza Artificiale”, dedicato al Machine Learning. In questo capitolo, esploreremo come l’intelligenza artificiale può “imparare” dai dati, un concetto che sta rivoluzionando numerosi campi, incluso quello giuridico. Per voi, futuri professionisti del diritto, comprendere il Machine Learning non è solo un esercizio accademico, ma una necessità pratica. Le tecnologie basate sul ML stanno già influenzando il settore legale, dalla ricerca giurisprudenziale automatizzata alla previsione degli esiti dei processi. La vostra capacità di navigare tra queste innovazioni sarà cruciale per la vostra carriera.</p>
<p>Inizieremo il nostro viaggio esplorando l’apprendimento supervisionato, dove gli algoritmi imparano da esempi etichettati. Vedremo come questo approccio possa essere applicato per classificare documenti legali o prevedere sentenze basandosi su casi precedenti. Ci addentreremo anche nell’affascinante mondo dell’apprendimento per rinforzo, particolarmente rilevante per la modellazione di strategie decisionali in contesti giuridici complessi.</p>
<p>Passeremo poi all’apprendimento non supervisionato, esplorando come gli algoritmi possano scoprire pattern nascosti in grandi volumi di dati legali non etichettati. Questa tecnica potrebbe rivoluzionare il modo in cui analizziamo la giurisprudenza e identifichiamo tendenze legali emergenti.</p>
<p>Un tema cruciale che affronteremo è quello dei bias negli algoritmi di ML. Come futuri giuristi, dovrete essere consapevoli di come i pregiudizi possano infiltrarsi nei sistemi automatizzati e delle implicazioni etiche e legali che ne derivano.</p>
<p>Infine, ci immergeremo nel mondo delle reti neurali e del deep learning, con un focus speciale sull’applicazione ai linguaggi naturali. Esploreremo i Large Language Models (LLM) come GPT e BERT, discutendo il loro potenziale rivoluzionario nel campo legale, dalla redazione di documenti all’assistenza nella ricerca legale.</p>
<p>Attraverso questo capitolo, non solo acquisirete una solida comprensione teorica del Machine Learning, ma imparerete anche ad applicare questi concetti utilizzando Python. Ogni sezione includerà esempi pratici e esercizi mirati al contesto legale, permettendovi di sperimentare direttamente con queste tecnologie.</p>
<p>Il nostro obiettivo è prepararvi ad essere non solo consumatori informati di tecnologie basate sul ML, ma anche potenziali innovatori nel vostro campo. Comprendere questi strumenti vi permetterà di anticipare come l’IA potrebbe influenzare la pratica legale futura e di contribuire attivamente a plasmare questo futuro.</p>
<p>Prepariamoci dunque a esplorare questo affascinante campo, dove la legge incontra l’intelligenza artificiale. Le competenze che acquisirete in questo capitolo non solo arricchiranno il vostro bagaglio tecnico, ma vi doteranno di una prospettiva unica e preziosa nel panorama giuridico in rapida evoluzione. ione di contenuti.</p>
<section id="apprendimento-supervisionato" class="level2">
<h2 class="anchored" data-anchor-id="apprendimento-supervisionato">Apprendimento Supervisionato</h2>
<section id="definizione-e-principi-di-base" class="level3">
<h3 class="anchored" data-anchor-id="definizione-e-principi-di-base">Definizione e Principi di Base</h3>
<p>L’apprendimento supervisionato è un sottoinsieme del machine learning che si occupa di costruire modelli predittivi utilizzando un dataset etichettato, dove ogni esempio di input è associato a un output corrispondente (l’etichetta). Il processo di apprendimento supervisionato può essere visto come una forma di mappatura funzionale (f: X Y), dove (X) rappresenta lo spazio degli input (caratteristiche o feature) e (Y) rappresenta lo spazio degli output (etichette). L’obiettivo principale è imparare una funzione (f) che, dato un nuovo input, sia in grado di predire l’output corretto.</p>
<p>Il processo di addestramento coinvolge due fasi principali: l’<strong>apprendimento</strong> e la <strong>generalizzazione</strong>. Durante la fase di apprendimento, il modello viene addestrato su un insieme di dati di addestramento, cercando di minimizzare la funzione di perdita, che misura la discrepanza tra le previsioni del modello e le etichette effettive. Successivamente, nella fase di generalizzazione, il modello viene testato su nuovi dati non visti per valutare la sua capacità di fare previsioni accurate al di fuori del set di addestramento.</p>
<p>I principi fondamentali che guidano l’apprendimento supervisionato includono la <strong>funzione di perdita</strong>, che determina quanto una previsione è lontana dal valore vero; l’<strong>ottimizzazione</strong>, che è il processo attraverso il quale il modello migliora le sue previsioni iterativamente; e il <strong>bias-variance tradeoff</strong>, che è il bilanciamento tra un modello troppo semplice (alto bias) e uno troppo complesso (alta varianza).</p>
</section>
<section id="classificazione" class="level3">
<h3 class="anchored" data-anchor-id="classificazione">Classificazione</h3>
<p>La classificazione è una tecnica di apprendimento supervisionato in cui l’obiettivo è assegnare una classe o etichetta specifica a un input, in base a un insieme di dati di addestramento. Esistono vari tipi di problemi di classificazione:</p>
<ul>
<li><p><strong>Classificazione binaria</strong>: Qui, l’output è limitato a due classi, come “sì” o “no”, “spam” o “non spam”. Questo tipo di problema è comune in scenari come la diagnosi medica (es. malato o non malato) e nella sicurezza informatica (es. email sicura o phishing).</p></li>
<li><p><strong>Classificazione multiclasse</strong>: In questo caso, l’output può appartenere a una di più classi (es. classificare un documento come “legale”, “finanziario” o “scientifico”). Le tecniche utilizzate possono includere approcci come la <strong>regressione logistica multinomiale</strong>, le <strong>reti neurali</strong> e le <strong>macchine a supporto di vettori (SVM)</strong>.</p></li>
<li><p><strong>Classificazione multilabel</strong>: Qui, un singolo input può essere associato a più classi contemporaneamente (es. un articolo di giornale che potrebbe essere classificato sia come “politico” che come “economico”). Tecniche come l’<strong>approccio One-vs-All</strong> e le reti neurali sono frequentemente utilizzate in questi contesti.</p></li>
</ul>
<p>Un punto di interesse particolare nella classificazione è il concetto di <strong>boundary decisionale</strong>. Questo rappresenta il confine nello spazio delle caratteristiche che separa le diverse classi. Nei modelli lineari, questo confine è una linea retta o un iperpiano, mentre nei modelli non lineari può assumere forme molto più complesse.</p>
</section>
<section id="regressione" class="level3">
<h3 class="anchored" data-anchor-id="regressione">Regressione</h3>
<p>La regressione è un tipo di problema di apprendimento supervisionato, focalizzato sulla previsione di un valore continuo piuttosto che su una classe discreta. A differenza della classificazione, dove l’output è un’etichetta, nella regressione l’output è un valore numerico che può variare su un intervallo continuo.</p>
<ul>
<li><p><strong>Regressione lineare</strong>: Il modello di regressione lineare è uno dei più semplici e intuitivi. Esso assume che esista una relazione lineare tra le caratteristiche dell’input e l’output. La formula generale per la regressione lineare semplice è <span class="math display">\[
y = \beta_0 + \beta_1x
,\]</span> dove <span class="math inline">\(\beta_0\)</span> è l’intercetta con l’asse delle ordinate e <span class="math inline">\(\beta_1\)</span> è la pendenza della retta.</p></li>
<li><p><strong>Regressione polinomiale</strong>: Quando la relazione tra le variabili non è lineare, si può ricorrere alla regressione polinomiale, che permette di modellare relazioni più complesse includendo termini polinomiali delle variabili indipendenti.</p></li>
<li><p><strong>Regressione multivariata</strong>: Questo tipo di regressione viene utilizzato quando si desidera prevedere l’output in base a più variabili indipendenti. È un’estensione naturale della regressione lineare e polinomiale.</p></li>
</ul>
<p>La <strong>regolarizzazione</strong> (es. Ridge e Lasso) è una tecnica comune utilizzata nella regressione per prevenire l’overfitting, imponendo una penalità alla complessità del modello, limitando così i valori dei coefficienti di regressione.</p>
</section>
<section id="algoritmi-principali-dellapprendimento-supervisionato" class="level3">
<h3 class="anchored" data-anchor-id="algoritmi-principali-dellapprendimento-supervisionato">Algoritmi Principali dell’Apprendimento Supervisionato</h3>
<p>L’apprendimento supervisionato si avvale di una vasta gamma di algoritmi che possono essere utilizzati per risolvere problemi sia di classificazione che di regressione. Ogni algoritmo ha caratteristiche specifiche che lo rendono più o meno adatto a particolari tipi di dati e problemi. Di seguito, verranno presentati alcuni dei principali algoritmi utilizzati nell’apprendimento supervisionato.</p>
<section id="regressione-lineare" class="level4">
<h4 class="anchored" data-anchor-id="regressione-lineare">Regressione Lineare</h4>
<p>La regressione lineare è uno degli algoritmi più semplici e ampiamente utilizzati per problemi di regressione. Assume una relazione lineare tra le variabili indipendenti e la variabile dipendente e cerca di trovare la retta (o l’iperpiano nel caso di più variabili indipendenti) che meglio approssima i dati. La semplicità della regressione lineare la rende facile da interpretare, ma la sua capacità di modellare solo relazioni lineari può limitare la sua applicabilità in scenari più complessi.</p>
</section>
<section id="regressione-logistica" class="level4">
<h4 class="anchored" data-anchor-id="regressione-logistica">Regressione Logistica</h4>
<p>La regressione logistica è un algoritmo di classificazione che viene utilizzato quando l’output è binario. A differenza della regressione lineare, la regressione logistica utilizza una funzione logistica (o sigmoide) per modellare la probabilità che un dato appartenga a una classe specifica. Questo approccio è ampiamente utilizzato per problemi come la classificazione di e-mail come “spam” o “non spam” o per la predizione di eventi binari (es. successo o fallimento di un’azione legale).</p>
</section>
<section id="alberi-di-decisione" class="level4">
<h4 class="anchored" data-anchor-id="alberi-di-decisione">Alberi di Decisione</h4>
<p>Gli alberi di decisione sono modelli non parametrici che possono essere utilizzati sia per la classificazione che per la regressione. Essi segmentano il dataset in sottogruppi omogenei attraverso una serie di decisioni basate sui valori delle caratteristiche. Ogni nodo dell’albero rappresenta una decisione basata su una caratteristica, e i rami rappresentano le possibili conseguenze di tale decisione. Gli alberi di decisione sono facili da interpretare e visualizzare, il che li rende particolarmente utili quando è necessaria una comprensione trasparente del processo decisionale. Tuttavia, gli alberi di decisione possono essere inclini all’overfitting, specialmente se non adeguatamente potati.</p>
</section>
<section id="random-forest" class="level4">
<h4 class="anchored" data-anchor-id="random-forest">Random Forest</h4>
<p>Il Random Forest è un metodo ensemble basato su alberi di decisione. Consiste in un insieme di alberi di decisione indipendenti addestrati su diverse porzioni del dataset (attraverso il bootstrapping) e utilizzando un sottoinsieme casuale di caratteristiche. Il risultato finale è ottenuto aggregando le previsioni di tutti gli alberi (es. tramite voto di maggioranza per la classificazione o media per la regressione). Questa tecnica riduce significativamente il rischio di overfitting rispetto a un singolo albero di decisione e migliora la precisione e la robustezza del modello.</p>
</section>
<section id="support-vector-machines-svm" class="level4">
<h4 class="anchored" data-anchor-id="support-vector-machines-svm">Support Vector Machines (SVM)</h4>
<p>Le Support Vector Machines (SVM) sono algoritmi molto potenti sia per la classificazione che per la regressione. Il loro obiettivo è trovare un iperpiano ottimale che separi i dati di diverse classi con il massimo margine possibile. Le SVM sono particolarmente efficaci in spazi ad alta dimensionalità e possono essere estese per gestire separazioni non lineari utilizzando il <strong>kernel trick</strong>, che permette di mappare i dati in uno spazio di dimensione superiore dove la separazione diventa lineare.</p>
</section>
<section id="k-nearest-neighbors-k-nn" class="level4">
<h4 class="anchored" data-anchor-id="k-nearest-neighbors-k-nn">k-Nearest Neighbors (k-NN)</h4>
<p>Il k-Nearest Neighbors (k-NN) è un algoritmo di classificazione e regressione basato su un’idea semplice ma efficace: per predire l’etichetta di un nuovo dato, si cercano i k punti più vicini nel dataset di addestramento e si assegna al nuovo dato la classe maggioritaria (nel caso di classificazione) o la media dei valori (nel caso di regressione). Il k-NN è molto intuitivo e non richiede una fase di addestramento, ma può diventare inefficiente con dataset molto grandi o in presenza di rumore.</p>
</section>
<section id="reti-neurali" class="level4">
<h4 class="anchored" data-anchor-id="reti-neurali">Reti Neurali</h4>
<p>Le reti neurali sono modelli ispirati al funzionamento del cervello umano e sono particolarmente potenti per la modellazione di relazioni non lineari complesse. Una rete neurale è composta da strati di nodi (neuroni) interconnessi, dove ciascun nodo applica una funzione non lineare ai dati in ingresso e trasmette il risultato ai nodi dello strato successivo. Le reti neurali possono essere utilizzate sia per la classificazione che per la regressione e sono alla base di tecniche avanzate come il <strong>deep learning</strong>.</p>
<ul>
<li><p><strong>Percettrone Multistrato (MLP)</strong>: È una delle architetture più semplici di reti neurali, composto da uno o più strati nascosti tra l’input e l’output. Il MLP è capace di apprendere rappresentazioni complesse dei dati, ma richiede un’attenta configurazione dei parametri e una grande quantità di dati per addestramento.</p></li>
<li><p><strong>Reti Neurali Convoluzionali (CNN)</strong>: Utilizzate principalmente per l’elaborazione di immagini, le CNN applicano convoluzioni ai dati in ingresso per estrarre automaticamente caratteristiche di alto livello. Sono particolarmente efficaci in problemi di riconoscimento di immagini e visione artificiale.</p></li>
<li><p><strong>Reti Neurali Ricorrenti (RNN)</strong>: Progettate per gestire dati sequenziali come testi o serie temporali, le RNN hanno connessioni che permettono l’uso di informazioni provenienti da precedenti stati dell’input. Questo le rende ideali per problemi come la modellazione del linguaggio naturale o la previsione di sequenze.</p></li>
</ul>
</section>
<section id="gradient-boosting-machines-gbm" class="level4">
<h4 class="anchored" data-anchor-id="gradient-boosting-machines-gbm">Gradient Boosting Machines (GBM)</h4>
<p>Il Gradient Boosting è una tecnica di ensemble che costruisce modelli in modo sequenziale, dove ogni nuovo modello cerca di correggere gli errori commessi dai modelli precedenti. I modelli individuali sono generalmente alberi di decisione semplici (stump), e il risultato finale è una somma ponderata di questi alberi. Algoritmi popolari come <strong>XGBoost</strong> e <strong>LightGBM</strong> sono varianti ottimizzate del Gradient Boosting, note per la loro efficacia e velocità, specialmente in competizioni di machine learning.</p>
</section>
<section id="naive-bayes" class="level4">
<h4 class="anchored" data-anchor-id="naive-bayes">Naive Bayes</h4>
<p>Il Naive Bayes è un algoritmo di classificazione basato sul teorema di Bayes, con l’assunzione “naive” che le caratteristiche siano indipendenti l’una dall’altra, una ipotesi raramente vera nel mondo reale. Nonostante questa assunzione, il Naive Bayes è sorprendentemente efficace, specialmente per problemi di classificazione testuale come la categorizzazione di documenti o l’analisi del sentiment.</p>
</section>
<section id="ensemble-learning" class="level4">
<h4 class="anchored" data-anchor-id="ensemble-learning">Ensemble Learning</h4>
<p>L’<strong>ensemble learning</strong> combina le previsioni di più modelli per ottenere un risultato finale più robusto e accurato. Oltre al Random Forest e al Gradient Boosting, altre tecniche di ensemble includono il <strong>bagging</strong> e lo <strong>stacking</strong>. Il bagging riduce la varianza addestrando lo stesso modello su diverse porzioni del dataset, mentre lo stacking combina le previsioni di diversi modelli tramite un meta-modello, che apprende a pesare le diverse previsioni.</p>
</section>
<section id="conclusioni" class="level4">
<h4 class="anchored" data-anchor-id="conclusioni">Conclusioni</h4>
<p>Ciascuno degli algoritmi discussi ha punti di forza e di debolezza che lo rendono più o meno adatto a particolari problemi di apprendimento supervisionato. La scelta dell’algoritmo più appropriato dipende dalla natura del problema, dalla quantità e qualità dei dati disponibili e dalle specifiche esigenze dell’applicazione. In contesti giuridici, dove la trasparenza e l’interpretabile sono spesso fondamentali, gli algoritmi semplici e interpretabili come gli alberi di decisione o la regressione logistica potrebbero essere preferibili, mentre in applicazioni più complesse come l’analisi di grandi volumi di dati testuali, algoritmi più sofisticati come le reti neurali o le tecniche di ensemble possono offrire prestazioni superiori.</p>
</section>
</section>
<section id="apprendimento-per-rinforzo" class="level3">
<h3 class="anchored" data-anchor-id="apprendimento-per-rinforzo">Apprendimento per Rinforzo</h3>
<section id="concetti-base" class="level4">
<h4 class="anchored" data-anchor-id="concetti-base">Concetti Base</h4>
<p>L’apprendimento per rinforzo (Reinforcement Learning, RL) si distingue dagli altri tipi di apprendimento supervisionato in quanto l’agente apprende attraverso l’interazione diretta con l’ambiente, senza avere accesso diretto a una serie di etichette corrette per ogni azione. In RL, l’agente prende decisioni sequenziali e riceve ricompense (o punizioni) che riflettono l’efficacia delle sue azioni. Il compito dell’agente è quindi quello di imparare una politica, o strategia, che massimizza la ricompensa totale nel tempo.</p>
</section>
<section id="agenti-ambiente-e-politiche" class="level4">
<h4 class="anchored" data-anchor-id="agenti-ambiente-e-politiche">Agenti, Ambiente e Politiche</h4>
<p>Gli elementi chiave nell’apprendimento per rinforzo includono:</p>
<ul>
<li><strong>Agente</strong>: L’entità che prende decisioni nell’ambiente.</li>
<li><strong>Ambiente</strong>: Il contesto in cui l’agente opera e da cui riceve feedback sotto forma di ricompense.</li>
<li><strong>Politica (Policy)</strong>: La strategia che l’agente segue per determinare quali azioni intraprendere in ogni stato.</li>
<li><strong>Funzione di valore (Value Function)</strong>: Una funzione che valuta l’utilità di essere in un certo stato, dato un insieme di azioni future possibili.</li>
<li><strong>Funzione di ricompensa (Reward Function)</strong>: Una funzione che fornisce un feedback immediato sulle azioni dell’agente.</li>
</ul>
</section>
<section id="algoritmi-principali-es.-q-learning-deep-q-networks" class="level4">
<h4 class="anchored" data-anchor-id="algoritmi-principali-es.-q-learning-deep-q-networks">Algoritmi Principali (es. Q-Learning, Deep Q-Networks)</h4>
<ul>
<li><p><strong>Q-Learning</strong>: È uno degli algoritmi di apprendimento per rinforzo più semplici e più conosciuti. Q-Learning si basa sull’apprendimento della funzione Q, che stima la qualità (o valore) di un’azione in un dato stato. L’agente utilizza questa funzione per decidere quali azioni intraprendere al fine di massimizzare la ricompensa cumulativa. Q-Learning è un algoritmo <strong>off-policy</strong>, il che significa che l’agente può apprendere la politica ottimale indipendentemente dalla politica attualmente seguita.</p></li>
<li><p><strong>Deep Q-Networks (DQN)</strong>: Estende Q-Learning utilizzando reti neurali profonde per approssimare la funzione Q, consentendo così di gestire ambienti con spazi di stato molto grandi o continui. Questo approccio è stato utilizzato con successo in diversi contesti, tra cui il superamento delle prestazioni umane in giochi complessi come Atari.</p></li>
</ul>
</section>
<section id="applicazioni" class="level4">
<h4 class="anchored" data-anchor-id="applicazioni">Applicazioni</h4>
<p>L’apprendimento per rinforzo è utilizzato in un’ampia varietà di applicazioni, che vanno dai giochi (es. scacchi, Go, e videogiochi come quelli sviluppati da OpenAI e DeepMind) alla robotica (es. robot che imparano a camminare o manipolare oggetti), fino a scenari come la guida autonoma. Nell’ambito giuridico, potrebbe essere applicato per ottimizzare flussi di lavoro complessi, simulare scenari di negoziazione o migliorare i processi decisionali attraverso simulazioni avanzate.</p>
</section>
</section>
<section id="overfitting-e-underfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting-e-underfitting">Overfitting e Underfitting</h3>
<p>L’overfitting e l’underfitting sono due delle principali problematiche che emergono nell’apprendimento supervisionato e possono influenzare significativamente la capacità di un modello di generalizzare su nuovi dati.</p>
<ul>
<li><p><strong>Overfitting</strong>: Si verifica quando un modello diventa troppo complesso, catturando non solo i pattern rilevanti nei dati di addestramento ma anche il rumore. Un modello overfit avrà prestazioni eccellenti sui dati di addestramento ma scarse prestazioni su dati nuovi e non visti. Questo problema può essere mitigato attraverso tecniche come la <strong>regolarizzazione</strong> (es. Lasso, Ridge), l’<strong>early stopping</strong> (interrompere l’addestramento prima che il modello inizi a memorizzare il rumore), e l’<strong>utilizzo di più dati</strong> o di <strong>modelli più semplici</strong>.</p></li>
<li><p><strong>Underfitting</strong>: Si verifica quando un modello è troppo semplice per rappresentare adeguatamente i dati. Un modello underfit avrà scarse prestazioni sia sui dati di addestramento che sui dati di test, poiché non riesce a catturare i pattern sottostanti. Per evitare l’underfitting, è necessario aumentare la complessità del modello o migliorare la qualità dei dati.</p></li>
</ul>
<p>L’obiettivo nella costruzione di un modello è trovare il giusto equilibrio tra bias e varianza, in modo da ottenere un modello che sia abbastanza complesso da catturare i pattern rilevanti nei dati senza diventare così complesso da catturare anche il rumore.</p>
</section>
<section id="valutazione-dei-modelli" class="level3">
<h3 class="anchored" data-anchor-id="valutazione-dei-modelli">Valutazione dei Modelli</h3>
<p>La valutazione dei modelli è un passo critico per garantire che un modello di apprendimento supervisionato sia non solo accurato ma anche robusto e generalizzabile a dati non visti. La scelta delle metriche di valutazione dipende dal tipo di problema (classificazione o regressione) e dalle specifiche esigenze dell’applicazione.</p>
<ul>
<li><p><strong>Valutazione nei problemi di classificazione</strong>:</p>
<ul>
<li><p><strong>Accuratezza</strong>: È la misura più semplice e rappresenta la proporzione di previsioni corrette sul totale delle previsioni. Tuttavia, in presenza di classi sbilanciate, l’accuratezza può essere ingannevole, poiché un modello che predice sempre la classe maggioritaria avrà un’accuratezza elevata anche se non è utile.</p></li>
<li><p><strong>Precisione e Recall</strong>: La precisione misura la proporzione di veri positivi rispetto al totale delle predizioni positive, mentre il recall misura la proporzione di veri positivi rispetto al totale dei veri positivi più i falsi negativi. Queste metriche sono particolarmente importanti quando ci sono costi associati ai falsi positivi o ai falsi negativi.</p></li>
<li><p><strong>F1-Score</strong>: È la media armonica tra precisione e recall e fornisce un singolo valore che rappresenta un buon compromesso tra queste due metriche. L’F1-score è utile in contesti dove è necessario bilanciare precisione e recall.</p></li>
<li><p><strong>AUC-ROC</strong>: La curva ROC (Receiver Operating Characteristic) e l’area sotto la curva ROC (AUC) sono strumenti grafici che rappresentano la capacità di un classificatore di distinguere tra le classi. AUC fornisce un valore che varia da 0 a 1, dove 1 rappresenta un classificatore perfetto e 0.5 rappresenta un classificatore casuale.</p></li>
<li><p><strong>Matrice di Confusione</strong>: Questa tabella riassume i veri positivi, falsi positivi, veri negativi e falsi negativi, offrendo una visione più dettagliata delle prestazioni del modello. È particolarmente utile per identificare se un modello ha bias specifici in determinate classi.</p></li>
</ul></li>
<li><p><strong>Valutazione nei problemi di regressione</strong>:</p>
<ul>
<li><p><strong>Errore Quadratico Medio (MSE)</strong>: Misura la media dei quadrati degli errori, penalizzando maggiormente gli errori grandi. È una delle metriche più utilizzate per problemi di regressione, ma è sensibile ai valori anomali.</p></li>
<li><p><strong>Errore Assoluto Medio (MAE)</strong>: Misura la media delle differenze assolute tra le previsioni e i valori reali. A differenza del MSE, il MAE è meno sensibile agli outlier, rendendolo una buona scelta quando si desidera una valutazione robusta.</p></li>
<li><p><strong>R² (R-quadrato)</strong>: Questa metrica rappresenta la proporzione della varianza nei dati di output che è spiegata dal modello. Un valore vicino a 1 indica che il modello spiega bene la varianza dei dati, mentre un valore vicino a 0 indica il contrario.</p></li>
</ul></li>
<li><p><strong>Cross-Validation</strong>:</p>
<p>La cross-validation è una tecnica statistica utilizzata per valutare la capacità di generalizzazione di un modello. Uno dei metodi più comuni è la <strong>k-fold cross-validation</strong>, dove il dataset viene diviso in k sottoinsiemi (folds), e il modello viene addestrato k volte, utilizzando ogni volta un diverso fold come set di test e gli altri k-1 come set di addestramento. La cross-validation aiuta a mitigare l’overfitting, fornendo una stima più affidabile delle prestazioni del modello.</p></li>
<li><p><strong>Bias e Varianza</strong>:</p>
<p>Il tradeoff tra bias e varianza è cruciale nella valutazione dei modelli. <strong>Bias</strong> alto indica che il modello è troppo rigido e non riesce a catturare la complessità dei dati (underfitting), mentre <strong>varianza</strong> alta indica che il modello è troppo sensibile ai dati di addestramento e non riesce a generalizzare (overfitting). Le tecniche di regolarizzazione, il tuning dei parametri e la scelta appropriata degli algoritmi possono aiutare a bilanciare bias e varianza per ottenere un modello ottimale.</p></li>
</ul>
<p><strong>Valutazione nei Problemi di Classificazione: Un Esempio di Recidiva Penale</strong></p>
<p>Consideriamo un modello utilizzato per predire la recidiva penale, dove l’obiettivo è determinare se un individuo sarà recidivo (1) o non recidivo (0) entro un certo periodo di tempo. Supponiamo di avere un dataset di test composto da 100 individui, e il modello ha prodotto le seguenti previsioni:</p>
<ul>
<li><strong>Veri Positivi (VP)</strong>: 40 (il modello ha correttamente predetto che 40 individui sarebbero recidivi)</li>
<li><strong>Falsi Positivi (FP)</strong>: 10 (il modello ha predetto che 10 individui sarebbero recidivi, ma in realtà non lo sono)</li>
<li><strong>Veri Negativi (VN)</strong>: 30 (il modello ha correttamente predetto che 30 individui non sarebbero recidivi)</li>
<li><strong>Falsi Negativi (FN)</strong>: 20 (il modello ha predetto che 20 individui non sarebbero recidivi, ma in realtà lo sono)</li>
</ul>
<p>Utilizziamo queste informazioni per calcolare alcune delle metriche di valutazione più comuni:</p>
<ul>
<li><p><strong>Accuratezza (Accuracy)</strong>: La proporzione di tutte le previsioni corrette sul totale delle osservazioni. <span class="math display">\[\text{Accuratezza} = \frac{VP + VN}{VP + FP + VN + FN} = \frac{40 + 30}{40 + 10 + 30 + 20} = \frac{70}{100} = 0{,}7\]</span></p>
<p>L’accuratezza del modello è 0,7, cioè il 70% delle previsioni sono corrette.</p></li>
<li><p><strong>Precisione (Precision)</strong>: La proporzione di veri positivi sul totale delle previsioni positive (veri positivi più falsi positivi). <span class="math display">\[\text{Precisione} = \frac{VP}{VP + FP} = \frac{40}{40 + 10} = \frac{40}{50} = 0{,}8\]</span> La precisione del modello è 0,8, ovvero l’80% delle predizioni di recidiva erano corrette.</p></li>
<li><p><strong>Recall (Sensibilità)</strong>: La proporzione di veri positivi sul totale dei veri recidivi (veri positivi più falsi negativi). <span class="math display">\[\text{Recall} = \frac{VP}{VP + FN} = \frac{40}{40 + 20} = \frac{40}{60} = 0{,}67\]</span> Il recall del modello è 0,67, cioè il 67% dei recidivi è stato correttamente identificato.</p></li>
<li><p><strong>F1-Score</strong>: La media armonica di precisione e recall, che fornisce un compromesso tra le due metriche. <span class="math display">\[\text{F1-Score} = 2 \times \frac{\text{Precisione} \times \text{Recall}}{\text{Precisione} + \text{Recall}} = 2 \times \frac{0{,}8 \times 0{,}67}{0{,}8 + 0{,}67} = 2 \times \frac{0{,}536}{1{,}47} \approx 0{,}73\]</span> L’F1-Score del modello è 0,73, che indica un buon bilanciamento tra precisione e recall.</p></li>
</ul>
<p><strong>Valutazione dei Risultati</strong>: Questo modello presenta una precisione alta, il che è positivo per evitare falsi allarmi, ma il recall è relativamente basso, suggerendo che alcuni recidivi non vengono identificati. In un contesto di recidiva penale, potrebbe essere necessario considerare un compromesso tra riduzione dei falsi positivi e aumento del recall, magari esplorando altre strategie di modellazione o modificando la soglia decisionale del modello.</p>
<p><strong>Valutazione nei Problemi di Regressione: Un Esempio di Previsione del valore di un Immobile</strong></p>
<p>Consideriamo ora un modello di regressione utilizzato per predire il valore di un immobile. Supponiamo di avere un dataset di test con i valori reali di 5 immobili e le previsioni del modello, come mostrato nella tabella seguente:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Immobile</th>
<th>Valore Reale (€)</th>
<th>Valore Predetto (€)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>300,000</td>
<td>310,000</td>
</tr>
<tr class="even">
<td>2</td>
<td>450,000</td>
<td>430,000</td>
</tr>
<tr class="odd">
<td>3</td>
<td>500,000</td>
<td>490,000</td>
</tr>
<tr class="even">
<td>4</td>
<td>400,000</td>
<td>420,000</td>
</tr>
<tr class="odd">
<td>5</td>
<td>350,000</td>
<td>345,000</td>
</tr>
</tbody>
</table>
<p>Utilizziamo questi dati per calcolare alcune metriche di valutazione:</p>
<ul>
<li><p><strong>Errore Assoluto Medio (MAE)</strong>: La media delle differenze assolute tra i valori predetti e quelli reali. <span class="math display">\[
\begin{split}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| = \\ \frac{|300-310|+|450-430|+|500-490|+|400-420|+|350-345|}{5}
\end{split}
\]</span> <span class="math display">\[
\text{MAE} = \frac{10,000 + 20,000 + 10,000 + 20,000 + 5,000}{5} = \frac{65,000}{5} = 13,000 \text{ €}
\]</span> L’errore assoluto medio è 13,000 €, indicando che, in media, le previsioni del modello differiscono dal valore reale di circa 13,000 €.</p></li>
<li><p><strong>Errore Quadratico Medio (MSE)</strong>: La media dei quadrati degli errori, che penalizza maggiormente gli errori più grandi. <span class="math display">\[
\begin{split}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \\ \frac{(300-310)^2+(450-430)^2+(500-490)^2+(400-420)^2+(350-345)^2}{5} \\
= \frac{(10,000)^2 + (-20,000)^2 + (-10,000)^2 + (-20,000)^2 + (5,000)^2}{5} \\
= \frac{100,000,000 + 400,000,000 + 100,000,000 + 400,000,000 + 25,000,000}{5} \\
= \frac{1,025,000,000}{5} = 205,000,000 \text{ €}^2
\end{split}
\]</span> L’errore quadratico medio è 205,000,000 €², che indica un’elevata sensibilità agli errori più grandi.</p></li>
<li><p><strong>Errore Quadratico Medio Radice (RMSE)</strong>: La radice quadrata dell’MSE, che riporta l’errore medio alla stessa scala delle variabili predette. <span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{205,000,000} \approx 14,318 \text{ €}
\]</span> Il valore dell’RMSE è 14,318 €, fornendo un’indicazione dell’errore medio sulle predizioni in termini di valore dell’immobile.</p></li>
<li><p><strong>R² (R-quadrato)</strong>: Misura la proporzione della varianza nei valori reali spiegata dal modello. <span class="math display">\[
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]</span> Dove <span class="math inline">\(\bar{y}\)</span> è il valore medio dei valori reali. Supponiamo che <span class="math inline">\(\bar{y} = 400,000\)</span> e che la somma dei quadrati delle differenze rispetto alla media sia 200,000,000,000 €. Allora: <span class="math display">\[
R^2 = 1 - \frac{1,025,000,000}{200,000,000,000} \approx 0{,}994
\]</span></p>
<p><strong>Valutazione dei Risultati</strong>: Il modello di regressione mostra una forte capacità di spiegare la varianza nei dati (R² = 0,994), il che suggerisce che è altamente predittivo per questo specifico dataset. Tuttavia, l’RMSE di 14,318 € e il MAE di 13,000 € indicano che, sebbene il modello sia generalmente accurato, ci sono ancora errori significativi nelle predizioni, che potrebbero essere rilevanti in contesti dove la precisione è cruciale, come nella determinazione del valore di un immobile per fini fiscali o di compravendita. La presenza di un MSE elevato suggerisce che il modello potrebbe essere sensibile agli outlier, e ulteriori indagini potrebbero essere necessarie per migliorare la robustezza del modello, magari esplorando tecniche di regolarizzazione o di gestione degli outlier.</p></li>
</ul>
<p>In ambito giuridico, la valutazione dei modelli può assumere un ruolo particolarmente delicato, poiché la precisione delle previsioni può influenzare decisioni importanti. Ad esempio, in un sistema predittivo per la recidiva criminale, è cruciale minimizzare sia i falsi positivi (persone etichettate erroneamente come ad alto rischio) che i falsi negativi (persone etichettate erroneamente come a basso rischio), utilizzando metriche come l’AUC-ROC e l’F1-score per garantire un equilibrio tra precisione e recall. In contesti dove i falsi positivi possono avere conseguenze legali severe, come nel rilevamento di frodi, l’accuratezza da sola potrebbe non essere sufficiente, rendendo necessaria una valutazione più sfumata attraverso una combinazione di metriche.</p>
</section>
</section>
<section id="apprendimento-non-supervisionato" class="level2">
<h2 class="anchored" data-anchor-id="apprendimento-non-supervisionato">Apprendimento Non Supervisionato</h2>
<p>L’apprendimento non supervisionato è un ramo del machine learning in cui i modelli vengono addestrati su dati senza etichette, ossia senza output conosciuti. L’obiettivo è scoprire strutture, pattern o relazioni nascoste all’interno dei dati. A differenza dell’apprendimento supervisionato, che richiede un dataset etichettato, l’apprendimento non supervisionato si concentra su come raggruppare dati simili o ridurre la complessità dei dati mantenendo le informazioni essenziali. Di seguito esamineremo i principali approcci e tecniche utilizzati in questo campo.</p>
<section id="clustering" class="level3">
<h3 class="anchored" data-anchor-id="clustering">Clustering</h3>
<p>Il clustering è una tecnica di apprendimento non supervisionato che mira a raggruppare i dati in gruppi (cluster) in base alla somiglianza tra gli elementi. Gli elementi all’interno di un cluster sono più simili tra loro rispetto a quelli di cluster differenti. Questa tecnica è ampiamente utilizzata per esplorare la struttura sottostante dei dati e per identificare segmenti naturali all’interno di un dataset.</p>
<p>Esistono vari metodi di clustering, tra cui:</p>
<ul>
<li><p><strong>K-means</strong>: Uno degli algoritmi di clustering più popolari, il k-means divide il dataset in k cluster, dove k è un numero predefinito. L’algoritmo funziona iterativamente, assegnando ogni punto dati al cluster il cui centroide è il più vicino e poi aggiornando i centroidi in base ai punti assegnati. Il processo continua fino a che i centroidi non cambiano più o le assegnazioni dei punti si stabilizzano. K-means è semplice ed efficace, ma può essere sensibile alla scelta di k e ai valori iniziali dei centroidi.</p></li>
<li><p><strong>Agglomerative Hierarchical Clustering</strong>: Questo approccio costruisce una gerarchia di cluster attraverso un processo iterativo in cui ogni punto dati inizia come un cluster separato, e a ogni passo, i due cluster più vicini vengono uniti. Questo processo continua fino a che tutti i punti dati non appartengono a un singolo cluster. Il risultato può essere rappresentato come un dendrogramma, che visualizza la struttura gerarchica dei cluster. Questo metodo è utile per esplorare la struttura dei dati a diversi livelli di granularità.</p></li>
<li><p><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong>: DBSCAN è un algoritmo di clustering basato sulla densità che identifica cluster di alta densità separati da aree di bassa densità. A differenza di k-means, DBSCAN non richiede di specificare il numero di cluster in anticipo e può identificare cluster di forma arbitraria, oltre a gestire outlier in modo naturale.</p></li>
</ul>
</section>
<section id="riduzione-della-dimensionalità" class="level3">
<h3 class="anchored" data-anchor-id="riduzione-della-dimensionalità">Riduzione della Dimensionalità</h3>
<p>La riduzione della dimensionalità è una tecnica che mira a ridurre il numero di variabili (o caratteristiche) nel dataset mantenendo la maggior parte dell’informazione rilevante. Questo è particolarmente utile quando si lavora con dati ad alta dimensionalità, dove un numero elevato di variabili può complicare l’analisi e aumentare il rischio di overfitting.</p>
<p>Alcuni dei metodi principali per la riduzione della dimensionalità includono:</p>
<ul>
<li><p><strong>Principal Component Analysis (PCA)</strong>: PCA è una tecnica matematica che trasforma i dati in un nuovo spazio di coordinate ridotto, dove le nuove variabili (componenti principali) sono combinazioni lineari delle variabili originali. Le componenti principali sono ordinate in modo tale che la prima componente catturi la massima varianza nei dati, la seconda componente catturi la seconda massima varianza, e così via. Riducendo il numero di componenti principali, PCA può ridurre la dimensionalità dei dati preservando gran parte dell’informazione originale.</p></li>
<li><p><strong>t-SNE (t-Distributed Stochastic Neighbor Embedding)</strong>: t-SNE è una tecnica di riduzione della dimensionalità non lineare che è particolarmente efficace per la visualizzazione di dati ad alta dimensionalità. Riduce i dati in uno spazio a 2 o 3 dimensioni, preservando le relazioni di vicinanza tra i punti dati, il che lo rende ideale per visualizzare cluster naturali nei dati.</p></li>
<li><p><strong>Autoencoder</strong>: Gli autoencoder sono reti neurali progettate per imparare una rappresentazione compressa dei dati. Sono composti da due parti: l’encoder, che riduce i dati in uno spazio a bassa dimensionalità, e il decoder, che ricostruisce i dati originali dalla rappresentazione compressa. Gli autoencoder sono particolarmente utili per la riduzione della dimensionalità in problemi complessi dove le relazioni tra le variabili non sono lineari.</p></li>
</ul>
</section>
<section id="algoritmi-principali" class="level3">
<h3 class="anchored" data-anchor-id="algoritmi-principali">Algoritmi Principali</h3>
<p>Nel contesto dell’apprendimento non supervisionato, ci sono diversi algoritmi che giocano un ruolo fondamentale. Alcuni di questi sono:</p>
<ul>
<li><p><strong>K-means</strong>: Come già discusso, k-means è un algoritmo di clustering ampiamente utilizzato grazie alla sua semplicità ed efficacia. Tuttavia, la scelta del numero di cluster (k) e la sensibilità ai valori iniziali possono influenzare significativamente i risultati.</p></li>
<li><p><strong>Gaussian Mixture Models (GMM)</strong>: GMM è un metodo di clustering che assume che i dati siano generati da una combinazione di distribuzioni gaussiane. Ogni cluster è modellato come una distribuzione gaussiana, e l’algoritmo cerca di trovare i parametri delle gaussiane che meglio spiegano la distribuzione dei dati. GMM è più flessibile di k-means in quanto può modellare cluster con forme ellittiche e non richiede che i cluster siano di forma sferica.</p></li>
<li><p><strong>Hierarchical Clustering</strong>: Questo algoritmo costruisce una gerarchia di cluster che possono essere esplorati a diversi livelli di granularità. Può essere agglomerativo o divisivo, offrendo una rappresentazione visiva della struttura dei cluster attraverso dendrogrammi.</p></li>
<li><p><strong>DBSCAN</strong>: Oltre al clustering basato sulla densità, DBSCAN è noto per la sua capacità di identificare outlier, rendendolo particolarmente utile in dataset con rumore o in cui i cluster non sono facilmente distinguibili.</p></li>
<li><p><strong>PCA</strong>: Sebbene originariamente progettato per la riduzione della dimensionalità, PCA è spesso utilizzato anche come metodo preliminare di analisi per comprendere la struttura dei dati e preparare i dati per ulteriori analisi di clustering.</p></li>
<li><p><strong>t-SNE</strong>: Questo algoritmo è particolarmente apprezzato per la visualizzazione di dati ad alta dimensionalità, specialmente in contesti dove è importante capire la struttura interna dei dati, come nel clustering o nell’identificazione di pattern nascosti.</p></li>
</ul>
</section>
<section id="applicazioni-1" class="level3">
<h3 class="anchored" data-anchor-id="applicazioni-1">Applicazioni</h3>
<p>L’apprendimento non supervisionato trova applicazione in una vasta gamma di settori e problemi, soprattutto in contesti in cui i dati non sono etichettati e l’obiettivo è scoprire strutture nascoste o ridurre la complessità dei dati. Alcune delle principali applicazioni includono:</p>
<ul>
<li><p><strong>Segmentazione del Mercato</strong>: Le tecniche di clustering come k-means e GMM sono utilizzate per segmentare i clienti in gruppi omogenei in base ai loro comportamenti o caratteristiche, permettendo strategie di marketing mirate e personalizzate.</p></li>
<li><p><strong>Analisi delle Reti Sociali</strong>: L’apprendimento non supervisionato può essere utilizzato per identificare comunità o gruppi di interesse all’interno di reti sociali, analizzando le connessioni tra individui o entità.</p></li>
<li><p><strong>Rilevamento delle Anomalie</strong>: Algoritmi come DBSCAN sono utilizzati per identificare outlier o anomalie nei dati, come transazioni fraudolente, guasti nei sistemi o attività insolite.</p></li>
<li><p><strong>Preprocessing dei Dati per Modelli Supervisionati</strong>: La riduzione della dimensionalità attraverso PCA o autoencoder è spesso utilizzata come fase di preprocessing per migliorare le prestazioni di modelli di apprendimento supervisionato, riducendo il rumore e la complessità dei dati.</p></li>
<li><p><strong>Visualizzazione dei Dati</strong>: Tecniche come t-SNE sono utilizzate per ridurre la dimensionalità dei dati ad alta complessità, facilitando la visualizzazione e l’interpretazione delle strutture interne dei dati, come cluster o pattern nascosti.</p></li>
</ul>
<p>In ambito giuridico, l’apprendimento non supervisionato può essere utilizzato per l’analisi di grandi volumi di documenti legali, la scoperta di pattern nei dati dei casi, o la segmentazione dei casi giudiziari per identificare tipologie ricorrenti e relazioni tra i casi. Queste tecniche forniscono strumenti potenti per esplorare e comprendere i dati in modo più profondo, senza la necessità di etichette predefinite.</p>
</section>
</section>
<section id="bias" class="level2">
<h2 class="anchored" data-anchor-id="bias">Bias</h2>
<p>Il bias nei modelli di machine learning è una questione critica, soprattutto in settori sensibili come il diritto, dove le decisioni automatizzate possono avere implicazioni significative su persone e gruppi. Il bias può portare a previsioni errate e ingiuste, perpetuando disuguaglianze sociali e legali. In questa sezione, esploreremo in dettaglio le diverse tipologie di bias, le cause e gli impatti che esse possono avere, e le tecniche per mitigare questi bias, con esempi pratici che illustrano il problema.</p>
<section id="tipologie-di-bias" class="level3">
<h3 class="anchored" data-anchor-id="tipologie-di-bias">Tipologie di Bias</h3>
<p>I modelli di machine learning possono essere affetti da vari tipi di bias, ciascuno con caratteristiche specifiche e potenziali impatti:</p>
<ul>
<li><p><strong>Bias di Selezione</strong>: Si verifica quando il dataset utilizzato per addestrare il modello non rappresenta adeguatamente la popolazione o il fenomeno che si intende modellare. Ad esempio, immaginate un modello di machine learning sviluppato per predire il successo di un’azione legale basato su dati storici. Se il dataset include solo casi di successo e non quelli falliti, il modello potrebbe sovrastimare le probabilità di successo. Supponiamo che su un campione di 1.000 casi, solo 100 siano stati inclusi nel dataset e questi siano stati scelti per la loro rilevanza legale; se 90 di questi 100 casi sono stati di successo, il modello potrebbe imparare che il successo è estremamente probabile (90%), ignorando che nella realtà solo 200 dei 1.000 casi totali erano di successo, riducendo la probabilità reale al 20%.</p></li>
<li><p><strong>Bias di Conferma</strong>: Questo tipo di bias emerge quando i dati o le caratteristiche selezionate per il modello confermano preconcetti o ipotesi preesistenti. Ad esempio, se un modello per la concessione del credito utilizza dati storici di prestiti concessi prevalentemente a individui di una determinata etnia o genere, potrebbe perpetuare lo stesso comportamento discriminatorio. Se nel dataset storico il 70% dei prestiti è stato concesso a uomini, il modello potrebbe imparare a favorire inconsciamente le richieste di prestito fatte da uomini, basando le sue decisioni su pattern storici piuttosto che su criteri obiettivi di solvibilità.</p></li>
<li><p><strong>Bias di Sopravvivenza</strong>: Questo bias si verifica quando l’analisi si basa solo sui dati relativi ai “sopravvissuti” a un determinato processo, ignorando i casi che non ce l’hanno fatta. Ad esempio, se un’analisi per determinare i fattori di successo per avviare uno studio legale si basa solo su studi legali che sono riusciti, si ignoreranno i casi di studi legali che hanno fallito, portando a una sovrastima delle caratteristiche di successo. Se su 1.000 studi legali, solo 100 sono rimasti attivi dopo 10 anni, ma l’analisi considera solo questi 100, il modello non riuscirà a catturare i motivi del fallimento degli altri 900.</p></li>
<li><p><strong>Bias Sistemico</strong>: Il bias sistemico riflette le disuguaglianze o le discriminazioni già presenti nei dati storici e nei sistemi sociali. Per esempio, un modello che predice la recidiva basato su dati storici che riflettono pratiche discriminatorie della polizia potrebbe perpetuare tali discriminazioni. Se i dati storici mostrano che un determinato gruppo etnico ha un tasso di arresto più alto a causa di pratiche di profilazione razziale, il modello potrebbe etichettare automaticamente questo gruppo come più a rischio di recidiva, senza considerare il bias nella raccolta dei dati.</p></li>
<li><p><strong>Bias Algoritmico</strong>: Questo tipo di bias è introdotto dall’algoritmo stesso, spesso a causa di obiettivi di ottimizzazione che non rappresentano adeguatamente il problema. Ad esempio, se un algoritmo di scoring creditizio è ottimizzato esclusivamente per ridurre i tassi di insolvenza, potrebbe penalizzare ingiustamente gruppi demografici che storicamente hanno avuto meno accesso al credito. Supponiamo che l’algoritmo tenda a ridurre il credito alle persone con un background socio-economico più basso per minimizzare il rischio; in questo caso, il bias si manifesta nel modo in cui l’algoritmo interpreta e valuta le caratteristiche socio-economiche.</p></li>
</ul>
</section>
<section id="cause-e-impatti-del-bias" class="level3">
<h3 class="anchored" data-anchor-id="cause-e-impatti-del-bias">Cause e Impatti del Bias</h3>
<p>Le cause del bias nei modelli di machine learning sono molteplici e possono derivare da diverse fasi del ciclo di vita del modello:</p>
<ul>
<li><p><strong>Dataset Non Rappresentativi</strong>: Uno dei motivi principali del bias è l’uso di dataset non rappresentativi della popolazione target. Se il modello è addestrato su dati che non riflettono tutte le possibili variabili del problema, esso sarà inevitabilmente biased. Ad esempio, se un modello di predizione della recidiva è addestrato principalmente su dati relativi a reati minori, potrebbe non essere accurato quando applicato a reati più gravi. Questo può portare a una sovrastima del rischio per alcuni individui, con conseguenti decisioni ingiuste.</p></li>
<li><p><strong>Scelte di Modellazione</strong>: Le decisioni prese durante la fase di modellazione, come la selezione delle caratteristiche o la scelta dell’algoritmo, possono introdurre bias. Se le caratteristiche scelte riflettono pregiudizi culturali o storici, il modello potrebbe imparare e riprodurre questi pregiudizi. Ad esempio, se si decide di includere la variabile “quartiere di residenza” in un modello per la concessione di prestiti, questo potrebbe introdurre bias se certi quartieri sono associati a gruppi etnici o socio-economici specifici.</p></li>
<li><p><strong>Feedback Loop</strong>: Un feedback loop si verifica quando le previsioni di un modello influenzano i dati futuri, rafforzando il bias. Ad esempio, un sistema di polizia predittiva che invia più pattuglie in quartieri già sorvegliati più intensamente potrebbe generare più arresti in quelle aree, portando a un ulteriore aumento della sorveglianza e a un rafforzamento del bias iniziale. Se un modello predice che una particolare zona ha un’alta probabilità di criminalità e quindi concentra lì le risorse di polizia, si avrà un numero maggiore di segnalazioni di crimini in quell’area, creando un ciclo che perpetua il bias.</p></li>
</ul>
<p>Gli impatti del bias possono essere devastanti, soprattutto in contesti dove le decisioni automatizzate influenzano direttamente le vite delle persone:</p>
<ul>
<li><p><strong>Discriminazione</strong>: Un modello biased può perpetuare o amplificare disuguaglianze esistenti, portando a decisioni discriminatorie. Ad esempio, se un modello di scoring creditizio discrimina ingiustamente contro minoranze etniche, potrebbe negare l’accesso al credito a persone altrimenti meritevoli. Supponiamo che in un dataset di 10.000 richieste di prestito, il modello respinga il 30% delle richieste da parte di una minoranza etnica specifica a causa di un bias nei dati storici; questo porterebbe a una discriminazione ingiustificata e potenzialmente illegale.</p></li>
<li><p><strong>Perdita di Fiducia</strong>: Se i sistemi di intelligenza artificiale sono percepiti come ingiusti o discriminatori, la fiducia del pubblico in questi sistemi può essere gravemente compromessa, limitando la loro accettazione e utilità. Ad esempio, se un sistema di giustizia predittiva è percepito come discriminatorio nei confronti di certi gruppi etnici, potrebbe sollevare preoccupazioni pubbliche e ridurre la legittimità delle decisioni automatizzate, portando a un rifiuto dell’uso di tali tecnologie.</p></li>
<li><p><strong>Conseguenze Legali</strong>: L’uso di modelli biased in decisioni legali può portare a contenziosi e danni reputazionali per le istituzioni che li utilizzano, oltre a violare normative e leggi sulla non discriminazione. Ad esempio, un’azienda che utilizza un modello di selezione del personale che favorisce inconsciamente candidati maschi rispetto a candidati femmine potrebbe essere esposta a cause legali per discriminazione di genere, con conseguenti danni economici e reputazionali.</p></li>
</ul>
</section>
<section id="tecniche-di-mitigazione-del-bias" class="level3">
<h3 class="anchored" data-anchor-id="tecniche-di-mitigazione-del-bias">Tecniche di Mitigazione del Bias</h3>
<p>Esistono diverse tecniche per mitigare il bias nei modelli di machine learning, che possono essere implementate in varie fasi del ciclo di vita del modello:</p>
<ul>
<li><p><strong>Raccolta e Preparazione dei Dati</strong>: Una delle tecniche più efficaci per mitigare il bias è assicurarsi che il dataset sia il più possibile rappresentativo della popolazione target. Ciò può richiedere la raccolta di dati aggiuntivi per garantire che tutti i gruppi siano equamente rappresentati. Ad esempio, se un dataset per la concessione di mutui mostra che solo il 10% dei richiedenti appartiene a una minoranza etnica, potrebbe essere utile raccogliere dati aggiuntivi per aumentare questa percentuale, riducendo così il bias nel modello.</p></li>
<li><p><strong>Pre-processing dei Dati</strong>: Prima dell’addestramento del modello, si possono applicare tecniche di pre-processing per ridurre il bias. Un esempio è l’eliminazione di caratteristiche correlate al bias (come genere o etnia) o la normalizzazione dei dati per garantire che nessun gruppo sia sovrarappresentato. Ad esempio, se si sta costruendo un modello di selezione del personale, si potrebbe rimuovere il campo “genere” per evitare che il modello impari a discriminare in base a esso.</p></li>
<li><p><strong>Modellazione In-process</strong>: Durante l’addestramento, possono essere applicate tecniche di regolarizzazione che penalizzano il modello se sfrutta caratteristiche correlate al bias. Ad esempio, si possono utilizzare obiettivi di equità, come la parità di trattamento tra diversi gruppi. Supponiamo di avere un modello di classificazione che tende a discriminare un gruppo specifico; applicando una regolarizzazione che penalizza il modello se questo gruppo ha un tasso di errore significativamente diverso dagli altri, è possibile ridurre il bias.</p></li>
<li><p><strong>Post-processing</strong>: Dopo l’addestramento, possono essere applicate tecniche di post-processing per correggere il bias nelle previsioni del modello. Un esempio è la calibrazione delle probabilità predette per garantire che le previsioni siano uniformi tra i diversi gruppi. Se un modello di scoring creditizio assegna punteggi più bassi a un determinato gruppo etnico, si può applicare un aggiustamento che uniformi i punteggi tra i gruppi, riducendo così il bias.</p></li>
<li><p><strong>Monitoraggio e Aggiornamento Continuo</strong>: Il bias può evolversi nel tempo man mano che cambiano i dati e le condizioni. È quindi essenziale monitorare continuamente i modelli e aggiornarli periodicamente per garantire che il bias non si reintroduca o peggiori. Ad esempio, un modello utilizzato per la concessione di prestiti potrebbe essere rivalutato ogni anno per verificare che non stia emergendo nuovo bias a causa di cambiamenti nei dati demografici o economici.</p></li>
<li><p><strong>Analisi di Impatto e Auditing</strong>: Condurre un’analisi di impatto e auditing regolare sui modelli di machine learning è fondamentale per identificare e correggere eventuali bias. Questo processo dovrebbe coinvolgere non solo esperti tecnici, ma anche stakeholder etici e legali per garantire che i modelli siano equi e conformi alle normative. Ad esempio, un modello utilizzato per predire la recidiva potrebbe essere sottoposto a una revisione annuale da parte di un comitato etico, che esamini l’equità delle previsioni e proponga modifiche se necessario.</p></li>
</ul>
<p><strong>Conclusione:</strong> La gestione del bias nei modelli di machine learning è essenziale per garantire che le applicazioni dell’IA, soprattutto in ambiti sensibili come il diritto, siano eque e giuste. Attraverso l’adozione di tecniche appropriate di mitigazione del bias, è possibile sviluppare modelli che non solo siano accurati, ma che rispettino anche i principi di equità e non discriminazione. Questi modelli possono quindi essere utilizzati in modo responsabile, minimizzando il rischio di perpetuare disuguaglianze e promuovendo decisioni più giuste e trasparenti.</p>
</section>
</section>
<section id="reti-neurali-1" class="level2">
<h2 class="anchored" data-anchor-id="reti-neurali-1">Reti Neurali</h2>
<p>Le reti neurali rappresentano una delle aree più affascinanti e potenti dell’intelligenza artificiale, capaci di modellare e risolvere problemi complessi che spaziano dalla classificazione di immagini alla generazione di testi. Originariamente ispirate al funzionamento del cervello umano, le reti neurali sono diventate un pilastro fondamentale nel campo del machine learning e del deep learning, offrendo soluzioni avanzate per una vasta gamma di applicazioni, compresi i sistemi giuridici. In questo paragrafo, esploreremo le basi delle reti neurali, partendo dai concetti fondamentali della regressione lineare e logistica, fino ad arrivare alle architetture più avanzate utilizzate nei moderni modelli di deep learning.</p>
<p>Il viaggio inizia con la <strong>Regressione Lineare e Logistica</strong> (Sezione 4.4.1), che, pur essendo modelli semplici, costituiscono le fondamenta su cui si costruiscono concetti più complessi delle reti neurali. La regressione lineare è utilizzata per problemi di previsione continua, mentre la regressione logistica è cruciale per problemi di classificazione binaria. Questi modelli, pur essendo relativamente semplici, offrono intuizioni fondamentali sulla modellazione dei dati e sulla costruzione di funzioni di previsione.</p>
<p>Proseguendo, discuteremo il <strong>Percettrone</strong> (Sezione 4.4.2), che è la forma più semplice di rete neurale e rappresenta un singolo neurone artificiale. Il percettrone è in grado di risolvere problemi di classificazione lineare ed è il punto di partenza per la comprensione delle reti neurali più complesse. Nonostante la sua semplicità, il percettrone ha un’importanza storica significativa, poiché ha introdotto il concetto di apprendimento supervisionato nelle reti neurali.</p>
<p>Successivamente, ci addentreremo nelle <strong>Reti Neurali Multistrato (MLP)</strong> (Sezione 4.4.3), che sono estensioni del percettrone e costituiscono il cuore delle reti neurali moderne. Un MLP è composto da più strati di neuroni, che permettono di modellare relazioni non lineari tra le variabili. Questa capacità di apprendere rappresentazioni complesse rende le MLP strumenti estremamente potenti per una vasta gamma di applicazioni, dalla predizione di valori continui alla classificazione di immagini.</p>
<p>Il capitolo esplorerà poi il mondo del <strong>Deep Learning</strong> (Sezione 4.4.4), una sottocategoria delle reti neurali che ha rivoluzionato il campo dell’intelligenza artificiale. Approfondiremo diverse <strong>Architetture</strong> (Sezione 4.4.4.1), come le Reti Neurali Convoluzionali (CNN), utilizzate principalmente per l’elaborazione delle immagini, le Reti Neurali Ricorrenti (RNN), ideali per l’elaborazione di dati sequenziali come il testo, e le Generative Adversarial Networks (GAN), che hanno aperto nuove frontiere nella generazione di contenuti. Inoltre, esploreremo le <strong>Tecniche di Addestramento</strong> (Sezione 4.4.4.2) che consentono alle reti neurali di apprendere in modo efficiente e accurato da grandi quantità di dati.</p>
<p>Infine, il capitolo si concentrerà sulle <strong>Applicazioni ai Linguaggi Naturali</strong> (Sezione 4.4.5), un’area in cui le reti neurali hanno fatto progressi straordinari. Discuteremo i <strong>Modelli di Linguaggio Preaddestrati</strong> (Sezione 4.4.5.1) e i <strong>Large Language Models (LLM)</strong> (Sezione 4.4.5.2), come GPT e BERT, che sono in grado di comprendere e generare testi in modo sorprendentemente umano. Esploreremo l’<strong>Architettura e le Caratteristiche</strong> (Sezione 4.4.5.2.1) di questi modelli, fornendo <strong>Esempi di LLM</strong> (Sezione 4.4.5.2.2) e analizzando le loro <strong>Applicazioni e Impatti</strong> (Sezione 4.4.5.2.3) nei campi del diritto, dell’educazione e oltre.</p>
<p>Questo capitolo fornirà agli studenti una comprensione profonda e completa delle reti neurali, evidenziando non solo i principi teorici, ma anche le applicazioni pratiche e le implicazioni etiche e sociali dell’uso di queste tecnologie avanzate.</p>
<section id="regressione-lineare-e-logistica" class="level3">
<h3 class="anchored" data-anchor-id="regressione-lineare-e-logistica">Regressione Lineare e Logistica</h3>
<p>Le tecniche di regressione lineare e logistica costituiscono le basi delle reti neurali e di molti altri algoritmi di machine learning. Questi modelli semplici ma potenti permettono di comprendere come le reti neurali apprendono dai dati e come vengono effettuate le previsioni.</p>
<section id="regressione-lineare-1" class="level4">
<h4 class="anchored" data-anchor-id="regressione-lineare-1">Regressione Lineare</h4>
<p>La <strong>regressione lineare</strong> è uno degli algoritmi più semplici e intuitivi per predire un valore continuo. L’idea centrale della regressione lineare è trovare una funzione lineare che meglio approssimi la relazione tra una o più variabili indipendenti (o caratteristiche) e una variabile dipendente.</p>
<p>Matematicamente, la regressione lineare può essere espressa come:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n\]</span></p>
<p>Dove: - <span class="math inline">\(\hat{y}\)</span> è il valore predetto della variabile dipendente. - <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> sono le variabili indipendenti (le caratteristiche). - <span class="math inline">\(\beta_0\)</span> è l’intercetta, che rappresenta il valore di <span class="math inline">\(\hat{y}\)</span> quando tutte le variabili indipendenti sono uguali a zero. - <span class="math inline">\(\beta_1, \beta_2, \dots, \beta_n\)</span> sono i coefficienti di regressione che indicano l’influenza di ciascuna variabile indipendente su <span class="math inline">\(\hat{y}\)</span>.</p>
<p>L’obiettivo della regressione lineare è trovare i valori dei coefficienti <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_n\)</span> che minimizzano la differenza tra i valori predetti <span class="math inline">\(\hat{y}\)</span> e i valori osservati <span class="math inline">\(y\)</span> nei dati. Questa differenza è spesso misurata utilizzando l’<strong>errore quadratico medio</strong> (Mean Squared Error, MSE), definito come:</p>
<p><span class="math display">\[MSE = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2\]</span></p>
<p>Dove: - <span class="math inline">\(m\)</span> è il numero di esempi nel dataset. - <span class="math inline">\(\hat{y}_i\)</span> è il valore predetto per l’i-esimo esempio. - <span class="math inline">\(y_i\)</span> è il valore osservato per l’i-esimo esempio.</p>
<p>La regressione lineare trova i coefficienti <span class="math inline">\(\beta\)</span> che minimizzano l’MSE, utilizzando tecniche di ottimizzazione come il <strong>metodo dei minimi quadrati</strong>.</p>
<p><strong>esempio di regressione lineare</strong>: si hanno a disposizione i valori di vari immobili di cui si conosce la superficie calpestabile. Si vuole prevedere/stimare il prezzo di un immobile di 150 m^2. Le ipotesi sono che tutti gli immobili sono nella stessa area urbana e sono di pari pregio e stato di conservazione. Inoltre, per comodità generemo i dati usando il generatore di numeri casuali della libreria <code>numpy</code>.</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generare dati di esempio</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Metri quadrati (variabile indipendente)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 50 immobili con una media di 100 m² e deviazione standard di 20 m²</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">20</span>, <span class="dv">50</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prezzo in migliaia di euro (variabile dipendente) con una certa </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># correlazione e aggiunta di rumore</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">200</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> X.flatten() <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">15</span>, X.shape[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Creare il modello di regressione lineare</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Predire i valori per la linea di regressione</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Grafico dei dati e del risultato della regressione</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Dati reali'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y_pred, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Regressione lineare'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Metri quadrati'</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Prezzo (migliaia di euro)'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Regressione Lineare: Predizione del Prezzo di un Immobile'</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Applicare i coefficienti della regressione a un immobile di 150 metri quadrati</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>square_meters <span class="op">=</span> np.array([[<span class="dv">150</span>]])</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>predicted_price <span class="op">=</span> model.predict(square_meters)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>model.coef_, model.intercept_, predicted_price[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(array([2.07730673]),
 np.float64(192.88465267224188),
 np.float64(504.48066278658195))</code></pre>
</div>
</div>
<p>Il modello di regressione lineare costruito ha prodotto i seguenti risultati:</p>
<ul>
<li><strong>Coefficiente della regressione <span class="math inline">\((\beta_1\)</span>)</strong>: 2.08 (approssimato), che rappresenta l’aumento medio del prezzo (in migliaia di euro) per ogni metro quadrato aggiuntivo.</li>
<li><strong>Intercetta <span class="math inline">\((\beta_0\)</span>)</strong>: 192.88 (approssimato), che rappresenta il prezzo di base in migliaia di euro quando l’immobile ha 0 metri quadrati.</li>
</ul>
<p>Utilizzando questi coefficienti per predire il prezzo di un immobile di 150 metri quadrati:</p>
<p><span class="math display">\[\text{Prezzo predetto} = 192.88 + 2.08 \times 150 \approx 504.48 \text{ migliaia di euro}\]</span></p>
<p>Quindi, il prezzo stimato per un immobile di 150 metri quadrati è di circa 504.48 migliaia di euro, ossia 504,480 euro.</p>
</section>
<section id="regressione-logistica-1" class="level4">
<h4 class="anchored" data-anchor-id="regressione-logistica-1">Regressione Logistica</h4>
<p>Mentre la regressione lineare è utilizzata per la previsione di valori continui, la <strong>regressione logistica</strong> è un modello di classificazione utilizzato quando l’obiettivo è prevedere una variabile dipendente binaria (cioè, che può assumere solo due valori, come 0 o 1).</p>
<p>La regressione logistica trasforma la previsione lineare utilizzando una <strong>funzione logistica</strong> (o sigmoide), che mappa i valori reali in un intervallo compreso tra 0 e 1. Questo intervallo può essere interpretato come una probabilità.</p>
<p>La funzione logistica è definita come:</p>
<p><span class="math display">\[\text{sigmoide}(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<p>Dove: - <span class="math inline">\(z\)</span> è la combinazione lineare delle caratteristiche, simile a quella usata nella regressione lineare: <span class="math display">\[z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n\]</span></p>
<p>Il valore predetto dalla regressione logistica, <span class="math inline">\(\hat{y}\)</span>, rappresenta la probabilità che la variabile dipendente assuma il valore 1, dato un insieme di caratteristiche <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>. Formalmente:</p>
<p><span class="math display">\[\hat{y} = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n)}}\]</span></p>
<p>Il modello viene addestrato utilizzando una funzione di perdita specifica, nota come <strong>log-loss</strong> o <strong>cross-entropy loss</strong>, che misura la differenza tra la probabilità predetta e il valore osservato:</p>
<p><span class="math display">\[\text{Log-Loss} = -\frac{1}{m} \sum_{i=1}^{m} \left[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right]\]</span></p>
<p>L’obiettivo è minimizzare la log-loss durante l’addestramento, regolando i coefficienti <span class="math inline">\(\beta\)</span> per migliorare la capacità del modello di distinguere tra le due classi.</p>
<p><strong>esempio di regressione logistica</strong>: si immagini di conoscere i dati relativi ad un coefficiente di rischio e alla probabilità di colpevolezza di un imputato. Per comodità i dati saranno simulati usando il generatore di numeri casuali di numpy con una distribuzione gaussiana.</p>
<div id="cell-8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Simuliamo un dataset per predire la probabilità che un imputato sia dichiarato colpevole</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Basandoci su un punteggio di rischio (ipotetico) su scala da 0 a 100</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generare dati di esempio</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Punteggio di rischio (variabile indipendente)</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 100 imputati con un punteggio medio di 50 e deviazione standard di 15</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.normal(<span class="dv">50</span>, <span class="dv">15</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Esito (colpevole = 1, non colpevole = 0), determinato da una funzione logistica con </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># un po' di rumore</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> (X.flatten() <span class="op">-</span> <span class="dv">50</span>)))) <span class="op">&gt;</span> np.random.rand(<span class="dv">100</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividere i dati in train e test</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Creare il modello di regressione logistica</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Predire i valori sul set di test</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>y_pred_prob <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Grafico dei dati e della curva di decisione</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Dati reali'</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_pred_prob, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Probabilità predetta'</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">sorted</span>(X_test.flatten()), <span class="bu">sorted</span>(y_pred_prob), color<span class="op">=</span><span class="st">'green'</span>, <span class="op">\</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">'Curva di decisione'</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Punteggio di rischio'</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probabilità di colpevolezza'</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Regressione Logistica: Predizione della Colpevolezza'</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrare la matrice di confusione</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, display_labels<span class="op">=\</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>                                      [<span class="st">"Non colpevole"</span>, <span class="st">"Colpevole"</span>], cmap<span class="op">=</span>plt.cm.Blues)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Matrice di Confusione'</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Questo ultimo grafico mostra la distribuzione delle previsioni del modello rispetto ai valori reali, evidenziando il numero di imputati correttamente classificati come “Colpevole” o “Non colpevole” e quelli classificati erroneamente. Questo strumento è utile per valutare le prestazioni del modello e comprendere meglio i suoi errori in un contesto giuridico.</p>
</section>
<section id="connessione-con-le-reti-neurali" class="level4">
<h4 class="anchored" data-anchor-id="connessione-con-le-reti-neurali">Connessione con le Reti Neurali</h4>
<p>Le reti neurali, in particolare quelle più semplici come il percettrone, possono essere viste come estensioni dei modelli di regressione lineare e logistica. Un neurone in una rete neurale esegue una combinazione lineare delle caratteristiche (come nella regressione lineare), seguita da una funzione di attivazione (come la funzione logistica nella regressione logistica). Questo permette alle reti neurali di apprendere rappresentazioni complesse e non lineari, rendendole strumenti estremamente potenti per la modellazione dei dati.</p>
<p>In sintesi, la comprensione della regressione lineare e logistica fornisce le basi per comprendere come funzionano le reti neurali, ponendo le fondamenta per concetti più avanzati come il deep learning. Questi modelli non solo offrono un’introduzione alla modellazione predittiva, ma forniscono anche intuizioni cruciali su come le reti neurali apprendono dai dati per fare previsioni.</p>
</section>
</section>
<section id="percettrone" class="level3">
<h3 class="anchored" data-anchor-id="percettrone">Percettrone</h3>
<p>Il percettrone è uno dei modelli più semplici e fondamentali delle reti neurali artificiali ed è stato introdotto da Frank Rosenblatt nel 1958. È considerato il progenitore delle reti neurali moderne e rappresenta un singolo neurone artificiale. Nonostante la sua semplicità, il percettrone ha avuto un impatto significativo nello sviluppo dell’intelligenza artificiale, dimostrando che le macchine potevano apprendere a classificare dati attraverso un processo di addestramento supervisionato.</p>
<section id="struttura-e-funzionamento" class="level4">
<h4 class="anchored" data-anchor-id="struttura-e-funzionamento">Struttura e Funzionamento</h4>
<p>Il percettrone è un modello di classificazione lineare che mira a separare i dati in due classi distinte (ad esempio, “positivo” e “negativo”). È composto da un insieme di ingressi, pesi associati a ciascun ingresso, un bias, una funzione di somma e una funzione di attivazione. Ogni ingresso rappresenta una caratteristica del dato da classificare.</p>
<p>Matematicamente, il funzionamento del percettrone può essere descritto come segue:</p>
<ol type="1">
<li><strong>Calcolo della somma ponderata</strong>: Ogni ingresso (x_i) viene moltiplicato per un peso <span class="math inline">\(w_i\)</span> corrispondente, e viene aggiunto un bias <span class="math inline">\(b\)</span>: <span class="math display">\[
z = \sum_{i=1}^{n} w_i \cdot x_i + b
\]</span> Dove:
<ul>
<li><span class="math inline">\(x_i\)</span> sono le caratteristiche in input.</li>
<li><span class="math inline">\(w_i\)</span> sono i pesi associati agli input.</li>
<li><span class="math inline">\(b\)</span> è il bias, un termine che permette di traslare la funzione di attivazione.</li>
</ul></li>
<li><strong>Funzione di attivazione</strong>: Il valore ottenuto dalla somma ponderata viene passato attraverso una funzione di attivazione. Nel percettrone classico, la funzione di attivazione è una funzione a soglia (o funzione di Heaviside): <span class="math display">\[
\hat{y} = \begin{cases}
1 &amp; \text{se } z \geq 0 \\
0 &amp; \text{se } z &lt; 0
\end{cases}
\]</span> Questa funzione determina l’output del percettrone: se la somma ponderata è maggiore o uguale a zero, l’output sarà 1 (classe positiva); altrimenti, sarà 0 (classe negativa).</li>
</ol>
</section>
<section id="addestramento-del-percettrone" class="level4">
<h4 class="anchored" data-anchor-id="addestramento-del-percettrone">Addestramento del Percettrone</h4>
<p>L’obiettivo dell’addestramento del percettrone è trovare i pesi <span class="math inline">\(w_i\)</span> e il bias <span class="math inline">\(b\)</span> che permettono al modello di classificare correttamente i dati. Questo processo avviene attraverso l’algoritmo di aggiornamento del percettrone, che segue questi passi:</p>
<ol type="1">
<li><p><strong>Inizializzazione</strong>: I pesi e il bias vengono inizializzati a valori casuali o a zero.</p></li>
<li><p><strong>Predizione</strong>: Per ogni esempio nel dataset di addestramento, il percettrone calcola l’output <span class="math inline">\(\hat{y}\)</span> utilizzando i pesi e il bias correnti.</p></li>
<li><p><strong>Aggiornamento</strong>: Se l’output <span class="math inline">\(\hat{y}\)</span> non corrisponde al valore atteso <span class="math inline">\(y\)</span> (cioè, il modello ha commesso un errore), i pesi e il bias vengono aggiornati:</p>
<p>$$ w_i w_i + w_i</p>
<p>b b + b $$ Dove</p>
<p><span class="math display">\[
\Delta w_i = \eta \cdot (y - \hat{y}) \cdot x_i
\]</span> e <span class="math display">\[
\Delta b = \eta \cdot (y - \hat{y})
\]</span> , con <span class="math inline">\(\eta\)</span> che rappresenta il tasso di apprendimento.</p></li>
<li><p><strong>Iterazione</strong>: Questo processo continua iterativamente fino a quando il modello non commette più errori o il numero massimo di iterazioni viene raggiunto.</p></li>
</ol>
</section>
<section id="limitazioni-e-applicazioni" class="level4">
<h4 class="anchored" data-anchor-id="limitazioni-e-applicazioni">Limitazioni e Applicazioni</h4>
<p>Una delle limitazioni principali del percettrone è che può risolvere solo problemi di classificazione linearmente separabili. Questo significa che se le classi non possono essere separate da una linea retta (o un iperpiano in dimensioni superiori), il percettrone non sarà in grado di classificare correttamente i dati. Tuttavia, l’introduzione di reti neurali multistrato (MLP) ha superato questa limitazione, permettendo di risolvere problemi più complessi.</p>
<p>Nonostante questa limitazione, il percettrone ha applicazioni pratiche in compiti di classificazione semplice e continua a essere un importante strumento educativo per comprendere i fondamenti delle reti neurali e del machine learning.</p>
<p><strong>esempio di Perceptrone</strong>: Esempio di applicazione del percettrone per predire l’esito di una causa legale, basandosi su due caratteristiche: la complessità del caso e l’esperienza dell’avvocato.</p>
<div id="cell-12" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Perceptron</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay, classification_report</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Creiamo un dataset simulato per un'applicazione giuridica</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Ad esempio, predire se una causa sarà vinta o persa basandosi su due caratteristiche </span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># legali (ad es., complessità del caso e esperienza dell'avvocato)</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generare dati di esempio</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>,<span class="op">\</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>                            n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># X, y = make_blobs(n_samples=100, centers=[[1,3], [3,1]], random_state=1)</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividere i dati in train e test</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Creare e addestrare il modello di Perceptrone</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>perc_model <span class="op">=</span> Perceptron(max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>perc_model.fit(X_train, y_train)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Predire sul set di test</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> perc_model.predict(X_test)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Grafico dei risultati</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati reali</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[y_test <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X_test[y_test <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'blue'</span>,<span class="op">\</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>             marker<span class="op">=</span><span class="st">'o'</span>, label<span class="op">=</span><span class="st">'Perso (Reale)'</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[y_test <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X_test[y_test <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'green'</span>,<span class="op">\</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>             marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Vinto (Reale)'</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Dati predetti</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[y_pred <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X_test[y_pred <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>,<span class="op">\</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>             marker<span class="op">=</span><span class="st">'o'</span>, facecolors<span class="op">=</span><span class="st">'none'</span>, label<span class="op">=</span><span class="st">'Perso (Predetto)'</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[y_pred <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X_test[y_pred <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'yellow'</span>,<span class="op">\</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>             marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'Vinto (Predetto)'</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Linea di decisione del perceptrone</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.linspace(X_test[:, <span class="dv">0</span>].<span class="bu">min</span>(), X_test[:, <span class="dv">0</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>decision_boundary <span class="op">=</span> <span class="op">-</span>(perc_model.coef_[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">*</span> x_values <span class="op">+</span> perc_model.intercept_[<span class="dv">0</span>]) <span class="op">\</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>                     <span class="op">/</span> perc_model.coef_[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values, decision_boundary, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>,<span class="op">\</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>          label<span class="op">=</span><span class="st">'Confine Decisionale'</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Complessità del caso'</span>)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Esperienza dell</span><span class="ch">\'</span><span class="st">avvocato'</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Perceptrone: Predizione della Vittoria o Sconfitta in un procedimento legale'</span>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrare la matrice di confusione e il rapporto di classificazione</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay.from_estimator(perc_model, X_test, y_test, display_labels<span class="op">=\</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>                                      [<span class="st">"Perso"</span>, <span class="st">"Vinto"</span>], cmap<span class="op">=</span>plt.cm.Blues)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Matrice di Confusione'</span>)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred, target_names<span class="op">=</span>[<span class="st">"Perso"</span>, <span class="st">"Vinto"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

       Perso       1.00      1.00      1.00        15
       Vinto       1.00      1.00      1.00        15

    accuracy                           1.00        30
   macro avg       1.00      1.00      1.00        30
weighted avg       1.00      1.00      1.00        30
</code></pre>
</div>
</div>
<p>Nel grafico sopra, i punti blu rappresentano i casi persi, mentre i punti verdi rappresentano i casi vinti, basati sui dati reali. I punti rossi e gialli rappresentano rispettivamente le predizioni del modello per i casi persi e vinti. La linea nera tratteggiata indica il confine decisionale del percettrone, che separa i casi predetti come vinti o persi.</p>
<p>Il modello ha eseguito una classificazione quasi perfetta su questo dataset di test, come evidenziato dalla matrice di confusione e dal rapporto di classificazione. Tutti i casi sono stati correttamente classificati, con una precisione, recall e F1-score di 1.00 per entrambe le classi (“Perso” e “Vinto”).</p>
<p>Questo risultato, pur essendo ideale, è tipico di dati sintetici e ben separabili linearmente, che spesso non riflettono la complessità dei casi reali. Tuttavia, dimostra come il percettrone possa essere utilizzato per costruire modelli di classificazione in ambiti giuridici, fornendo una base per decisioni automatizzate in scenari meno complessi. Questo esempio ci aiuta a comprendere l’importanza di valutare le performance del modello in contesti realistici e di considerare l’uso di modelli più complessi, come le reti neurali multistrato, quando le classi non sono facilmente separabili linearmente.</p>
</section>
</section>
<section id="deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning">Deep learning</h3>
<p>Il deep learning è una sottocategoria avanzata del machine learning che sfrutta reti neurali profonde per affrontare problemi complessi, caratterizzati da grandi quantità di dati e dalla necessità di apprendere rappresentazioni astratte e stratificate delle informazioni. Il viaggio nel deep learning inizia spesso con le Reti Neurali Multistrato (MLP), che rappresentano il primo passo verso la comprensione delle reti neurali profonde.</p>
<section id="reti-neurali-multistrato-mlp" class="level4">
<h4 class="anchored" data-anchor-id="reti-neurali-multistrato-mlp">Reti Neurali Multistrato (MLP)</h4>
<p>Le Reti Neurali Multistrato (MLP = Multi Layer Perceptron) sono una forma di rete neurale feedforward, composte da uno strato di input, uno o più strati nascosti e uno strato di output. Le MLP sono in grado di apprendere rappresentazioni non lineari dei dati grazie all’uso di funzioni di attivazione non lineari nei neuroni dei loro strati nascosti. Sebbene siano efficaci per molti compiti, le MLP tradizionali hanno una capacità limitata di affrontare problemi particolarmente complessi, poiché sono generalmente composte da pochi strati nascosti.</p>
<p>L’addestramento delle MLP avviene attraverso l’algoritmo di backpropagation, che calcola e minimizza l’errore del modello aggiornando i pesi dei collegamenti tra i neuroni. Questa tecnica è fondamentale non solo per le MLP, ma anche per le reti neurali più complesse utilizzate nel deep learning.</p>
<section id="architettura-delle-mlp" class="level5">
<h5 class="anchored" data-anchor-id="architettura-delle-mlp">Architettura delle MLP</h5>
<p>Un MLP può essere configurato in diverse architetture a seconda del problema da risolvere. La configurazione più comune è quella con un singolo strato di input, uno o più strati nascosti e uno strato di output. Ogni neurone in uno strato è collegato a tutti i neuroni dello strato successivo, rendendo la rete completamente connessa.</p>
<ul>
<li><p><strong>Reti con un solo strato nascosto</strong>: Questo è il tipo più semplice di MLP, in cui un singolo strato nascosto è sufficiente per risolvere problemi relativamente semplici o linearmente separabili con una funzione di attivazione non lineare.</p></li>
<li><p><strong>Reti con più strati nascosti</strong>: Quando i dati presentano una complessità maggiore, un MLP con più strati nascosti può catturare pattern più complessi. Ogni strato aggiuntivo consente alla rete di apprendere rappresentazioni intermedie che possono essere utilizzate per ottenere una predizione finale più accurata.</p></li>
<li><p><strong>Deep MLP</strong>: Quando il numero di strati nascosti aumenta significativamente, la rete viene considerata “profonda” (deep). Questi modelli, sebbene potenti, richiedono una maggiore attenzione durante l’addestramento per evitare problemi come l’overfitting o la vanishing gradient problem.</p></li>
</ul>
</section>
<section id="funzioni-di-attivazione" class="level5">
<h5 class="anchored" data-anchor-id="funzioni-di-attivazione">Funzioni di Attivazione</h5>
<p>Le funzioni di attivazione sono cruciali per introdurre la non linearità nelle reti neurali, permettendo al modello di apprendere pattern complessi. Le funzioni di attivazione più comuni negli strati nascosti includono:</p>
<ul>
<li><p><strong>Sigmoide</strong>: Questa funzione mappa qualsiasi valore reale in un intervallo compreso tra 0 e 1, ed è definita come: <span class="math display">\[\text{sigmoide}(z) = \frac{1}{1 + e^{-z}}\]</span> La funzione sigmoide è utile quando si ha bisogno di un output probabilistico, ma può soffrire del problema della vanishing gradient, che rende difficile l’addestramento di reti profonde.</p></li>
<li><p><strong>ReLU (Rectified Linear Unit)</strong>: Una delle funzioni di attivazione più popolari, definita come: <span class="math display">\[\text{ReLU}(z) = \max(0, z)\]</span> ReLU è ampiamente utilizzata perché risolve in parte il problema della vanishing gradient, accelerando l’addestramento delle reti profonde. Tuttavia, può soffrire del problema della “morte dei neuroni”, dove i neuroni possono rimanere bloccati su zero.</p></li>
<li><p><strong>Tanh</strong>: Un’alternativa alla funzione sigmoide, mappa i valori in un intervallo tra -1 e 1, ed è definita come: <span class="math display">\[\text{tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span> Tanh è spesso preferita alla sigmoide per la sua capacità di centrare i dati attorno a zero, migliorando la convergenza del modello.</p></li>
</ul>
</section>
<section id="funzioni-di-attivazione-degli-strati-di-uscita" class="level5">
<h5 class="anchored" data-anchor-id="funzioni-di-attivazione-degli-strati-di-uscita">Funzioni di Attivazione degli Strati di Uscita</h5>
<p>La scelta della funzione di attivazione nello strato di uscita dipende dal tipo di problema che il modello deve risolvere:</p>
<ul>
<li><p><strong>Classificazione binaria</strong>: Si utilizza comunemente la funzione sigmoide nello strato di uscita per ottenere una probabilità che l’output appartenga a una delle due classi.</p></li>
<li><p><strong>Classificazione multiclasse</strong>: La funzione softmax è preferita, poiché mappa i valori di output in un intervallo compreso tra 0 e 1 e la loro somma è 1, fornendo quindi una distribuzione di probabilità tra le diverse classi: <span class="math display">\[\text{softmax}(z_j) = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}\]</span> Dove <span class="math inline">\(z_j\)</span> è l’output per la j-esima classe.</p></li>
<li><p><strong>Regressione</strong>: Per problemi di regressione, in genere non si applica alcuna funzione di attivazione nell’ultimo strato (o si utilizza l’identità) per mantenere l’output come un valore reale continuo.</p></li>
</ul>
</section>
<section id="funzioni-di-errore" class="level5">
<h5 class="anchored" data-anchor-id="funzioni-di-errore">Funzioni di Errore</h5>
<p>Le funzioni di errore (o funzioni di perdita) misurano la discrepanza tra l’output predetto dal modello e il valore reale, guidando così il processo di apprendimento:</p>
<ul>
<li><p><strong>Errore Quadratico Medio (MSE)</strong>: Utilizzato per problemi di regressione, è definito come: <span class="math display">\[MSE = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2\]</span> Dove <span class="math inline">\(y_i\)</span> è il valore reale e <span class="math inline">\(\hat{y}_i\)</span> è il valore predetto.</p></li>
<li><p><strong>Cross-Entropy Loss</strong>: Utilizzata per la classificazione, particolarmente con softmax o sigmoide, misura la distanza tra le distribuzioni di probabilità: <span class="math display">\[\text{Cross-Entropy} = -\sum_{i=1}^n y_i \log(\hat{y}_i)\]</span></p></li>
</ul>
</section>
<section id="algoritmo-di-backpropagation" class="level5">
<h5 class="anchored" data-anchor-id="algoritmo-di-backpropagation">Algoritmo di Backpropagation</h5>
<p>Il backpropagation è l’algoritmo chiave che permette l’addestramento delle reti neurali multistrato. Funziona in due fasi:</p>
<ol type="1">
<li><p><strong>Feedforward</strong>: I dati vengono propagati in avanti attraverso la rete fino a generare un output.</p></li>
<li><p><strong>Calcolo della perdita e propagazione all’indietro</strong>: L’errore viene calcolato confrontando l’output predetto con il valore reale. Questo errore viene poi propagato all’indietro attraverso la rete, calcolando il gradiente della funzione di perdita rispetto ai pesi della rete. I pesi vengono aggiornati utilizzando il gradient descent, minimizzando così la funzione di perdita.</p></li>
</ol>
<p>Il backpropagation è iterativo e viene eseguito per molte epoche fino a quando il modello non raggiunge un livello accettabile di precisione.</p>
</section>
<section id="esempio-di-rete-mlp" class="level5">
<h5 class="anchored" data-anchor-id="esempio-di-rete-mlp">Esempio di Rete MLP</h5>
<p>Applicazione di una rete neurale multistrato (MLP) per la predizione dell’esito di un caso giudiziario basandosi su tre caratteristiche: complessità del caso, esperienza dell’avvocato, e importanza mediatica. La rete è composta da:</p>
<ul>
<li><strong>Strato di input</strong>: Tre neuroni, ciascuno corrispondente a una delle caratteristiche del dataset (complessità del caso, esperienza dell’avvocato, importanza mediatica).</li>
<li><strong>Strati nascosti</strong>: Due strati nascosti, il primo con 10 neuroni e il secondo con 5 neuroni, che permettono alla rete di apprendere rappresentazioni più complesse dei dati.</li>
<li><strong>Strato di output</strong>: Un singolo neurone di output, utilizzato per la classificazione binaria (vittoria o sconfitta del caso).</li>
</ul>
<p>Ogni neurone in un determinato strato è connesso a tutti i neuroni dello strato successivo, consentendo il flusso delle informazioni attraverso la rete durante l’addestramento e la predizione. Una rappresentazione grafica di una rete neurale può essere ottenuta usando la libreria <code>networkx</code> in Python. Qui di seguito si propone una funzione Python che disegna il grafo di una rete MLP data la sua descrizione in termini di numero di neuroni di ingresso, numero di strati e numero di neuroni per ogni strato e numero di neuroni di uscita.</p>
<div id="cell-15" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> networkx <span class="im">as</span> nx</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Funzione per disegnare una rappresentazione grafica della rete MLP</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_mlp(hidden_layers, input_size, output_size):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> nx.DiGraph()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    layer_sizes <span class="op">=</span> [input_size] <span class="op">+</span> <span class="bu">list</span>(hidden_layers) <span class="op">+</span> [output_size]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posizionamento dei nodi</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> {}</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    n_layers <span class="op">=</span> <span class="bu">len</span>(layer_sizes)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    v_spacing <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    h_spacing <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="bu">float</span>(<span class="bu">max</span>(layer_sizes))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creazione dei nodi</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, layer_size <span class="kw">in</span> <span class="bu">enumerate</span>(layer_sizes):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        layer_top <span class="op">=</span> v_spacing <span class="op">*</span> (layer_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(layer_size):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            pos[<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> (i, layer_top <span class="op">-</span> v_spacing <span class="op">*</span> j)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            G.add_node(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creazione degli archi</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (layer_size_a, layer_size_b) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(layer_sizes[:<span class="op">-</span><span class="dv">1</span>], layer_sizes[<span class="dv">1</span>:])):</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(layer_size_a):</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(layer_size_b):</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                G.add_edge(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>j<span class="sc">}</span><span class="ss">'</span>, <span class="ss">f'</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Disegna il grafico</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    nx.draw(G, pos<span class="op">=</span>pos, with_labels<span class="op">=</span><span class="va">False</span>, arrows<span class="op">=</span><span class="va">False</span>, node_size<span class="op">=</span><span class="dv">300</span>, node_color<span class="op">=</span><span class="st">"lightblue"</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Etichette</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(input_size):</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        pos[<span class="ss">f'0-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> (pos[<span class="ss">f'0-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">0</span>] <span class="op">-</span> <span class="fl">0.1</span>, pos[<span class="ss">f'0-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">1</span>])</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        plt.text(pos[<span class="ss">f'0-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">0</span>], pos[<span class="ss">f'0-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">1</span>], <span class="ss">f'Input </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, horizontalalignment<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(output_size):</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        pos[<span class="ss">f'</span><span class="sc">{</span>n_layers<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>] <span class="op">=</span> (pos[<span class="ss">f'</span><span class="sc">{</span>n_layers<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.1</span>, pos[<span class="ss">f'</span><span class="sc">{</span>n_layers<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">1</span>])</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        plt.text(pos[<span class="ss">f'</span><span class="sc">{</span>n_layers<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">0</span>], pos[<span class="ss">f'</span><span class="sc">{</span>n_layers<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>][<span class="dv">1</span>], <span class="ss">f'Output </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, horizontalalignment<span class="op">=</span><span class="st">'left'</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">"Rappresentazione Grafica della Rete MLP"</span>)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Usando la funzione appena introdotta possiamo disegnare la rete MLP usata nell’esempio come segue:</p>
<div id="cell-17" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Parametri della rete MLP utilizzata nell'esempio</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>hidden_layers <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">5</span>)  <span class="co"># Due strati nascosti con 10 e 5 neuroni rispettivamente</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Tre caratteristiche in input</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Un neurone di output (classificazione binaria)</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Disegnare la rappresentazione della rete MLP</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>draw_mlp(hidden_layers, input_size, output_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>L’implementazione in Python della rete MLP per il nostroo problema di classificazione è la seguente:</p>
<div id="cell-19" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay, classification_report</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simuliamo un dataset per predire se un caso giudiziario sarà vinto o perso basandosi su tre caratteristiche</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ad esempio, complessità del caso, esperienza dell'avvocato, e importanza mediatica</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Generare dati di esempio</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">200</span>, n_features<span class="op">=</span><span class="dv">3</span>, n_informative<span class="op">=</span><span class="dv">3</span>, n_redundant<span class="op">=</span><span class="dv">0</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividere i dati in train e test</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Creare e addestrare un modello MLP</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), max_iter<span class="op">=</span><span class="dv">1000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>mlp_model.fit(X_train, y_train)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predire sul set di test</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> mlp_model.predict(X_test)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrare la matrice di confusione</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay.from_estimator(mlp_model, X_test, y_test, display_labels<span class="op">=</span>[<span class="st">"Perso"</span>, <span class="st">"Vinto"</span>], cmap<span class="op">=</span>plt.cm.Blues)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Matrice di Confusione'</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizzare il rapporto di classificazione</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, y_pred, target_names<span class="op">=</span>[<span class="st">"Perso"</span>, <span class="st">"Vinto"</span>])</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\lcapitanio\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\neural_network\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

       Perso       0.94      0.91      0.93        35
       Vinto       0.88      0.92      0.90        25

    accuracy                           0.92        60
   macro avg       0.91      0.92      0.91        60
weighted avg       0.92      0.92      0.92        60
</code></pre>
</div>
</div>
<p><strong>Analisi dei Risultati</strong></p>
<ol type="1">
<li><p><strong>Matrice di Confusione</strong>: La matrice di confusione mostra le prestazioni del modello nella classificazione dei casi giudiziari come “Vinto” o “Perso”. Nel set di test, il modello ha classificato correttamente la maggior parte dei casi, con solo pochi errori. La matrice di confusione indica che il modello ha identificato con una buona precisione sia i casi vinti che quelli persi.</p></li>
<li><p><strong>Rapporto di Classificazione</strong>:</p>
<ul>
<li><strong>Precisione</strong>: La precisione per i casi persi è del 94%, mentre per i casi vinti è dell’88%. Questo significa che quando il modello prevede un caso come “Perso”, nel 94% dei casi ha ragione, mentre per i casi “Vinto”, la precisione è leggermente inferiore.</li>
<li><strong>Recall</strong>: La recall per i casi persi è del 91% e per i casi vinti è del 92%. Questo indica che il modello è riuscito a identificare correttamente la maggior parte dei casi vinti e persi.</li>
<li><strong>F1-score</strong>: L’F1-score, che rappresenta un bilanciamento tra precisione e recall, è del 93% per i casi persi e del 90% per i casi vinti, riflettendo una buona performance complessiva del modello.</li>
</ul></li>
</ol>
<p><strong>Osservazioni</strong>: - Il modello ha raggiunto un’accuratezza complessiva del 92%, che è un buon risultato considerando che i dati generati non sono perfettamente separabili. - È importante notare che il modello non ha raggiunto il valore di convergenza entro il numero massimo di iterazioni impostato (1000), come indicato dall’avviso di convergenza. Questo suggerisce che con ulteriori iterazioni o con l’ottimizzazione dei parametri del modello, le prestazioni potrebbero migliorare ulteriormente. - In sintesi, l’MLP si è dimostrato efficace nel classificare correttamente i casi giudiziari in base alle caratteristiche fornite, anche in presenza di dati non perfettamente distinti. - Questo esempio mostra il potenziale delle reti neurali multistrato per applicazioni giuridiche, come la predizione degli esiti legali, pur sottolineando l’importanza di una corretta configurazione e addestramento del modello per ottenere i migliori risultati possibili.</p>
</section>
</section>
<section id="architetture-di-reti-neurali-profonde" class="level4">
<h4 class="anchored" data-anchor-id="architetture-di-reti-neurali-profonde">Architetture di Reti Neurali Profonde</h4>
<p>Le reti neurali profonde rappresentano una specializzazione e un’estensione delle MLP. Queste reti, spesso costituite da decine o centinaia di strati nascosti, sono in grado di apprendere rappresentazioni molto più complesse e astratte rispetto alle MLP tradizionali. Ogni strato di una rete profonda elabora i dati in modo più dettagliato, consentendo al modello di catturare caratteristiche gerarchiche dei dati, come pattern semplici nei primi strati e strutture più complesse nei successivi.</p>
<p><strong>Reti Neurali Convoluzionali (CNN)</strong></p>
<p>Le reti neurali convoluzionali (CNN) sono una classe specializzata di reti neurali artificiali, particolarmente efficaci nell’elaborazione di dati strutturati a griglia, come le immagini. Il termine “convoluzionale” è fondamentale per comprendere il loro funzionamento unico.</p>
<p><strong>Cos’è la Convoluzione?</strong></p>
<p>La convoluzione è un’operazione matematica che sta alla base di queste reti. In termini semplici, consiste nell’applicare un filtro (o kernel) a una porzione dell’input, facendolo “scorrere” su tutta l’immagine. Questo processo può essere immaginato come una lente che si muove sull’immagine, focalizzandosi su piccole aree alla volta.</p>
<pre class="mermaid"><code>graph LR
    A[Immagine Input] --&gt; B[Applicazione Filtro]
    B --&gt; C[Feature Map]
    B --&gt; D[Scorrimento]
    D --&gt; |Ripeti| B
    C --&gt; E[Attivazione]
    E --&gt; F[Pooling]
    F --&gt; G[Prossimo Strato]</code></pre>
<ol type="1">
<li><strong>Filtri e Feature Maps</strong>:
<ul>
<li>I filtri sono matrici di pesi che vengono applicati all’input.</li>
<li>Ogni filtro è progettato per rilevare specifiche caratteristiche (come bordi, curve, o texture).</li>
<li>Il risultato dell’applicazione di un filtro è chiamato “feature map”.</li>
</ul></li>
<li><strong>Processo di Scorrimento</strong>:
<ul>
<li>Il filtro si muove sistematicamente attraverso l’immagine, pixel per pixel.</li>
<li>Ad ogni posizione, esegue una moltiplicazione elemento per elemento e una somma.</li>
<li>Questo crea una nuova rappresentazione dell’immagine che evidenzia certe caratteristiche.</li>
</ul></li>
<li><strong>Vantaggi della Convoluzione</strong>:
<ul>
<li><strong>Invarianza spaziale</strong>: La stessa caratteristica può essere rilevata ovunque nell’immagine.</li>
<li><strong>Parametri condivisi</strong>: I pesi del filtro sono riutilizzati, riducendo il numero totale di parametri.</li>
<li><strong>Gerarchia di features</strong>: Strati più profondi combinano features semplici in rappresentazioni più complesse.</li>
</ul></li>
</ol>
<p>Le CNN impilano multiple operazioni di convoluzione, alternate con funzioni di attivazione non lineari (come ReLU) e strati di pooling. Questa architettura permette alla rete di costruire una comprensione gerarchica dell’input, partendo da caratteristiche semplici negli strati iniziali (come bordi e texture) fino a concetti più astratti negli strati più profondi (come forme complesse e oggetti interi). Grazie a questa struttura “convoluzionale”, le CNN sono eccezionalmente efficaci in compiti come il riconoscimento di immagini, la detection di oggetti, e la segmentazione semantica, superando spesso le capacità umane in questi domini.</p>
<p><strong>Esempio di creazione, addestramento e test di una rete convoluzionale</strong> Applicazione di una rete convoluzionale (CNN) al dataset CIFAR-10 in Python con la libreria <a href="https://keras.io/">Keras</a> (che fa parte di <a href="https://www.tensorflow.org/?hl=it">TensorFlow</a>). Il dataset <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> contiene 60.000 immagini a colori di 32x32 pixel suddivise in 10 classi, con 50.000 immagini per il training e 10.000 immagini per il test. Descrizione del Codice: - Caricamento e Preprocessamento dei Dati:Carichiamo il dataset CIFAR-10 già disponibile in Keras e lo dividiamo in training set e test set. Normalizziamo i valori dei pixel per far sì che siano compresi tra 0 e 1. - Visualizzazione delle Immagini:Visualizziamo alcune immagini del training set con i rispettivi nomi delle classi per avere un’idea del tipo di dati. - Creazione della Rete Convoluzionale:Utilizziamo tre blocchi di convoluzione seguiti da pooling, aumentando il numero di filtri in ogni strato. Alla fine della rete convoluzionale, utilizziamo uno strato Flatten per trasformare i dati in un formato compatibile con uno strato completamente connesso. Lo strato completamente connesso finale ha 10 unità, corrispondenti alle 10 classi del dataset CIFAR-10. - Compilazione del Modello: Utilizziamo l’ottimizzatore Adam e la funzione di perdita SparseCategoricalCrossentropy. - Allenamento del Modello: Alleniamo il modello per 10 epoche e visualizziamo la precisione e la perdita sia sul training set che sul validation set. - Valutazione: Alla fine, valutiamo il modello sul test set e stampiamo l’accuratezza. - Risultato Atteso: Il modello raggiungerà un’accuratezza intorno al 70-75% sul test set, a seconda della configurazione e dell’hardware utilizzato.</p>
<div id="cell-23" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importiamo le librerie necessarie</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> datasets, layers, models</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Carichiamo il dataset CIFAR-10</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> datasets.cifar10.load_data()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizziamo i valori dei pixel a un intervallo [0, 1]</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>train_images, test_images <span class="op">=</span> train_images <span class="op">/</span> <span class="fl">255.0</span>, test_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostriamo alcune immagini di esempio dal dataset CIFAR-10</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'Aereo'</span>, <span class="st">'Auto'</span>, <span class="st">'Uccello'</span>, <span class="st">'Gatto'</span>, <span class="st">'Cervo'</span>, <span class="st">'Cane'</span>, <span class="st">'Rana'</span>, <span class="st">'Cavallo'</span>, <span class="st">'Nave'</span>, <span class="st">'Camion'</span>]</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">5</span>, <span class="dv">5</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    plt.xticks([])</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    plt.yticks([])</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">False</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    plt.imshow(train_images[i])</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Le etichette nel dataset sono numeri interi, dobbiamo mapparle con i nomi delle classi</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(class_names[train_labels[i][<span class="dv">0</span>]])</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Creiamo il modello della rete convoluzionale</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.Sequential()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Primo strato convoluzionale</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>model.add(layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)))</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>model.add(layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>)))</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Secondo strato convoluzionale</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>model.add(layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>model.add(layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>)))</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Terzo strato convoluzionale</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>model.add(layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten (convertiamo i dati in un vettore 1D)</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>model.add(layers.Flatten())</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Strato completamente connesso</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>model.add(layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Strato di output con 10 unità (una per ciascuna classe del CIFAR-10)</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>model.add(layers.Dense(<span class="dv">10</span>))</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Riepilogo della rete</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Compiliamo il modello</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Alleniamo il modello</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(test_images, test_labels))</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizziamo i grafici di accuratezza e perdita</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Accuracy'</span>)</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Val Accuracy'</span>)</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Loss'</span>)</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Val Loss'</span>)</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Valutiamo il modello sui dati di test</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(test_images, test_labels, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Accuratezza sul test set: </span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
170498071/170498071 ━━━━━━━━━━━━━━━━━━━━ 36s 0us/step</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\lcapitanio\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\layers\convolutional\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv2d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">30</span>, <span style="color: #00af00; text-decoration-color: #00af00">30</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     │           <span style="color: #00af00; text-decoration-color: #00af00">896</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">15</span>, <span style="color: #00af00; text-decoration-color: #00af00">15</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">18,496</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)       │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)       │        <span style="color: #00af00; text-decoration-color: #00af00">36,928</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1024</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)             │        <span style="color: #00af00; text-decoration-color: #00af00">65,600</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">650</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">122,570</span> (478.79 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">122,570</span> (478.79 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step - accuracy: 0.3583 - loss: 1.7137 - val_accuracy: 0.5424 - val_loss: 1.2747
Epoch 2/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step - accuracy: 0.5744 - loss: 1.1928 - val_accuracy: 0.6309 - val_loss: 1.0408
Epoch 3/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step - accuracy: 0.6488 - loss: 1.0025 - val_accuracy: 0.6566 - val_loss: 0.9731
Epoch 4/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 18s 12ms/step - accuracy: 0.6862 - loss: 0.8934 - val_accuracy: 0.6846 - val_loss: 0.9034
Epoch 5/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 21s 13ms/step - accuracy: 0.7160 - loss: 0.8103 - val_accuracy: 0.7025 - val_loss: 0.8726
Epoch 6/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step - accuracy: 0.7379 - loss: 0.7481 - val_accuracy: 0.6917 - val_loss: 0.8936
Epoch 7/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 18s 12ms/step - accuracy: 0.7555 - loss: 0.6952 - val_accuracy: 0.7042 - val_loss: 0.8687
Epoch 8/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 19s 12ms/step - accuracy: 0.7762 - loss: 0.6473 - val_accuracy: 0.7215 - val_loss: 0.8195
Epoch 9/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 19s 12ms/step - accuracy: 0.7879 - loss: 0.6004 - val_accuracy: 0.7213 - val_loss: 0.8363
Epoch 10/10
1563/1563 ━━━━━━━━━━━━━━━━━━━━ 19s 12ms/step - accuracy: 0.8032 - loss: 0.5645 - val_accuracy: 0.7246 - val_loss: 0.8529</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="3-machine learning_files/figure-html/cell-9-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 - 3s - 8ms/step - accuracy: 0.7246 - loss: 0.8529

Accuratezza sul test set: 0.7246000170707703</code></pre>
</div>
</div>
<ul>
<li><p><strong>Reti Neurali Ricorrenti (RNN)</strong>: Le RNN sono un’estensione delle MLP per dati sequenziali, come testi o segnali audio. Grazie alle connessioni ricorrenti, le RNN possono mantenere una memoria delle informazioni precedenti nella sequenza, rendendole ideali per compiti che richiedono la modellazione del contesto temporale. Tuttavia, le RNN tradizionali soffrono del problema del vanishing gradient, che può ostacolare l’apprendimento di dipendenze a lungo termine nelle sequenze. Per superare questa limitazione, sono state sviluppate varianti come le LSTM (Long Short-Term Memory) e le GRU (Gated Recurrent Unit), che migliorano la capacità della rete di apprendere e mantenere informazioni su lunghe sequenze temporali.</p></li>
<li><p><strong>Reti Generative Adversariali (GAN)</strong>: Le GAN rappresentano un’architettura avanzata che combina due reti, una generativa e una discriminativa, in un quadro competitivo. Queste reti sono in grado di generare nuovi dati simili a quelli reali, estendendo le capacità delle MLP in modi creativi e innovativi. La rete generativa tenta di produrre dati falsi che siano indistinguibili dai dati reali, mentre la rete discriminativa cerca di distinguere tra dati reali e falsi. Questo approccio ha portato a notevoli innovazioni nella generazione di immagini realistiche, video, musica e persino testo, aprendo nuove possibilità nel campo della creatività artificiale e della simulazione.</p></li>
</ul>
</section>
<section id="tecniche-di-addestramento-per-reti-profonde" class="level4">
<h4 class="anchored" data-anchor-id="tecniche-di-addestramento-per-reti-profonde">Tecniche di Addestramento per Reti Profonde</h4>
<p>L’addestramento delle reti profonde è più complesso rispetto a quello delle MLP a causa della maggiore profondità e del numero di parametri coinvolti. Il processo di addestramento utilizza algoritmi di ottimizzazione come la discesa del gradiente, ma con alcune sfide specifiche:</p>
<ul>
<li><p><strong>Problema del Vanishing Gradient</strong>: Nelle reti molto profonde, i gradienti calcolati durante la backpropagation possono diventare molto piccoli, impedendo l’aggiornamento efficace dei pesi nei primi strati della rete. Questo problema è particolarmente critico nelle RNN, dove la propagazione dei gradienti attraverso molteplici passi temporali può portare alla perdita di informazioni utili. Per mitigare questo problema, si utilizzano funzioni di attivazione come ReLU, che mantengono gradienti più ampi, e tecniche come il batch normalization, che stabilizza e accelera il processo di addestramento.</p></li>
<li><p><strong>Batch Normalization</strong>: Questa tecnica normalizza gli input a ciascuno strato per avere una media zero e una varianza unitaria, riducendo così il rischio di gradienti esplosivi o vanishing e migliorando la stabilità dell’addestramento. Il batch normalization è ampiamente utilizzato nelle reti profonde, poiché permette un addestramento più efficiente e riduce la sensibilità agli iperparametri, facilitando l’uso di learning rate più elevati.</p></li>
<li><p><strong>Dropout</strong>: Per prevenire l’overfitting, una delle tecniche più comuni è il dropout, che consiste nel disattivare casualmente alcuni neuroni durante l’addestramento, impedendo alla rete di dipendere troppo da specifiche connessioni. Questo forza la rete a generalizzare meglio, migliorando le sue prestazioni su dati mai visti. Durante la fase di inferenza, tutti i neuroni vengono utilizzati, ma i pesi sono scalati per mantenere la coerenza delle attivazioni.</p></li>
</ul>
</section>
<section id="tecniche-di-ottimizzazione-dei-parametri-delle-reti-profonde" class="level4">
<h4 class="anchored" data-anchor-id="tecniche-di-ottimizzazione-dei-parametri-delle-reti-profonde">Tecniche di Ottimizzazione dei Parametri delle Reti Profonde</h4>
<p>Oltre alle tecniche di addestramento, le reti profonde richiedono l’uso di tecniche avanzate di ottimizzazione per gestire la complessità e migliorare la convergenza:</p>
<ul>
<li><p><strong>Algoritmi di Ottimizzazione</strong>: Sebbene la discesa del gradiente stocastica (SGD) sia l’approccio di base, varianti più avanzate come Adam (Adaptive Moment Estimation) e RMSprop sono ampiamente utilizzate. Adam, in particolare, combina i vantaggi di AdaGrad (che adatta il learning rate per ogni parametro) e RMSprop (che mantiene un learning rate efficiente per ogni parametro), risultando in una convergenza più rapida e stabile anche in reti molto profonde.</p></li>
<li><p><strong>Learning Rate Scheduling</strong>: Il learning rate, ossia la velocità con cui vengono aggiornati i pesi, è un parametro critico che influisce sulla velocità e sull’efficacia dell’addestramento. Tecniche come il learning rate scheduling permettono di iniziare l’addestramento con un learning rate elevato, che viene ridotto man mano che il modello si avvicina a una soluzione ottimale. Questo aiuta a trovare il minimo globale della funzione di perdita più rapidamente.</p></li>
<li><p><strong>Early Stopping</strong>: Per evitare l’overfitting durante l’addestramento, l’early stopping monitora la performance del modello su un set di validazione e interrompe l’addestramento quando le prestazioni iniziano a peggiorare. Questo evita che la rete apprenda troppo i dettagli del set di addestramento, migliorando la generalizzazione.</p></li>
<li><h4 id="uso-di-modelli-pre-addestrati" class="anchored">Uso di modelli pre-addestrati</h4></li>
</ul>
<p>L’addestramento delle reti neurali profonde richiede notevoli risorse computazionali e dataset di grandi dimensioni, rendendo i costi in termini di tempo e potenza di calcolo molto elevati. Per ridurre questi costi, l’uso di modelli pre-addestrati rappresenta una soluzione efficace, poiché consente di sfruttare reti già addestrate su ampi dataset e adattarle a specifici problemi con un processo noto come fine-tuning. Ciò permette di evitare il lungo e dispendioso processo di addestramento da zero, ottenendo comunque prestazioni eccellenti.</p>
<p>Le collezioni di modelli pre-addestrati disponibili su piattaforme come <a href="https://www.tensorflow.org/hub?hl=it">TensorFlow Hub</a>, <a href="https://pytorch.org/hub/">PyTorch Hub</a> e <a href="https://huggingface.co/models">Hugging Face Model Hub</a> offrono reti avanzate, già ottimizzate, come ResNet, EfficientNet per la visione e BERT, GPT per il linguaggio. Questi modelli, addestrati su dataset estesi, possono essere facilmente utilizzati per applicazioni specifiche con poche risorse computazionali aggiuntive, rendendo il processo più accessibile ed economico senza sacrificare la qualità delle prestazioni.</p>
<p><strong>esempio di uso di rete pre-addestrata</strong></p>
<p>Usiamo una rete pre-addestrata per il compito di classificazione di aimmagini che abbiamo visto nel paragrafo 4.4.3.2. Per utilizzare una rete preaddestrata con il dataset CIFAR-10, possiamo sfruttare un modello preaddestrato su ImageNet, come ResNet50, e adattarlo al nostro compito tramite il fine-tuning. L’idea è quella di caricare il modello preaddestrato, congelare i pesi degli strati inferiori e modificare solo gli ultimi strati per adattare il modello alle 10 classi di CIFAR-10.</p>
<p>Ecco un esempio di di fine tuning di una rete pre-addestrata utilizzando Keras e TensorFlow.</p>
<div id="cell-25" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> cifar10</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.applications <span class="im">import</span> VGG16</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense, Flatten, Dropout, GlobalAveragePooling2D</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Carica e prepara il dataset CIFAR-10</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> cifar10.load_data()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">'float32'</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> tf.keras.utils.to_categorical(y_train, <span class="dv">10</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> tf.keras.utils.to_categorical(y_test, <span class="dv">10</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Data augmentation</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    width_shift_range<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    height_shift_range<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>datagen.fit(x_train)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Carica il modello VGG16 preaddestrato senza i layer fully connected</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> VGG16(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">False</span>, input_shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>))</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Fine-tuning: sblocca gli ultimi blocchi convoluzionali</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers[<span class="op">-</span><span class="dv">4</span>:]:</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Crea un nuovo modello aggiungendo layer personalizzati</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    base_model,</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>    GlobalAveragePooling2D(),</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.5</span>),</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.5</span>),</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Compila il modello</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.0001</span>),</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Addestra il modello</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(datagen.flow(x_train, y_train, batch_size<span class="op">=</span><span class="dv">16</span>),</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>                    steps_per_epoch<span class="op">=</span><span class="bu">len</span>(x_train) <span class="op">//</span> <span class="dv">16</span>,</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(x_test, y_test),</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>                    verbose<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Valuta il modello sul set di test</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(x_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuratezza sul set di test: </span><span class="sc">{</span>test_acc<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Supponiamo che 'history', 'model', 'x_test', 'y_test' siano già definiti dal codice precedente</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Nomi delle classi CIFAR-10</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Grafico dell'accuratezza</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'accuracy'</span>], label<span class="op">=</span><span class="st">'Train Accuracy'</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span><span class="st">'Validation Accuracy'</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Accuracy'</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Grafico della loss</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Train Loss'</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Model Loss'</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Matrice di confusione</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(x_test)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>y_pred_classes <span class="op">=</span> np.argmax(y_pred, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>y_true_classes <span class="op">=</span> np.argmax(y_test, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_true_classes, y_pred_classes)</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>sns.heatmap(cm, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'d'</span>, cmap<span class="op">=</span><span class="st">'Blues'</span>, xticklabels<span class="op">=</span>class_names, yticklabels<span class="op">=</span>class_names)</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Confusion Matrix'</span>)</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted'</span>)</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True'</span>)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Visualizzazione di 20 esempi di immagini classificate</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>n_images <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(x_test)), n_images, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(indices):</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">4</span>, <span class="dv">5</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> x_test[idx]</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>    plt.imshow(img)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>    true_label <span class="op">=</span> class_names[y_true_classes[idx]]</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>    pred_label <span class="op">=</span> class_names[y_pred_classes[idx]]</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> <span class="st">'green'</span> <span class="cf">if</span> true_label <span class="op">==</span> pred_label <span class="cf">else</span> <span class="st">'red'</span></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"True: </span><span class="sc">{</span>true_label<span class="sc">}</span><span class="ch">\n</span><span class="ss">Pred: </span><span class="sc">{</span>pred_label<span class="sc">}</span><span class="ss">"</span>, color<span class="op">=</span>color)</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Stampa l'accuratezza complessiva</span></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(x_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuratezza complessiva sul set di test: </span><span class="sc">{</span>test_acc<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="applicazioni-e-sfide" class="level4">
<h4 class="anchored" data-anchor-id="applicazioni-e-sfide">Applicazioni e Sfide</h4>
<p>Le applicazioni del deep learning sono vaste e coprono molte aree, dalla visione artificiale all’elaborazione del linguaggio naturale. In ambito giuridico, le reti profonde possono essere utilizzate per l’analisi predittiva, la classificazione automatica di documenti legali e l’estrazione di informazioni da grandi volumi di testo. Tuttavia, l’implementazione del deep learning richiede una grande quantità di dati e risorse computazionali, oltre a una profonda comprensione delle reti neurali per evitare problemi di interpretabilità e bias. Nonostante queste sfide, il deep learning continua a spingere i confini dell’intelligenza artificiale, offrendo soluzioni avanzate a problemi complessi che erano precedentemente irrisolvibili.</p>
<p>Machine learning e linguaggi naturali</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./2-algoritmi.html" class="pagination-link" aria-label="Algoritmi">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Algoritmi</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./a1-elementi-di-Python.html" class="pagination-link" aria-label="Elementi di Python">
        <span class="nav-page-text"><span class="chapter-title">Elementi di Python</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Laboratorio di Intelligenza Artificiale (1e) scritto da Luciano Capitanio.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Draft - Licenza Apache ver. 2</p>
</div>
  </div>
</footer>




</body></html>