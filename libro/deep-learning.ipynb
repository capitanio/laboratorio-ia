{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep learning\n",
        "\n",
        "## Laboratorio Python\n",
        "\n",
        "### Esperimento 1: Rappresentazione grafiche di reti neurali multistrato\n",
        "\n",
        "In questo esperimento vogliamo programmare una funzione in grado di  realizzare una rappresentazione grafica di una rete neurale. A tale scopo adotteremo la libreria Python **networkx**. \n",
        "La funzione draw_mlp riceve in ingresso la descrizione della rete multistrato in termini di numero di neuroni di ingresso, numero di strati e numero di neuroni per ogni strato e numero di neuroni di uscita.\n",
        "Il risultato della funzione è il disegno del grafo della rete MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Funzione per disegnare una rappresentazione grafica della rete MLP\n",
        "def draw_mlp(hidden_layers, input_size, output_size):\n",
        "    G = nx.DiGraph()\n",
        "    layer_sizes = [input_size] + list(hidden_layers) + [output_size]\n",
        "    \n",
        "    # Posizionamento dei nodi\n",
        "    pos = {}\n",
        "    n_layers = len(layer_sizes)\n",
        "    v_spacing = 1\n",
        "    \n",
        "    # Creazione dei nodi\n",
        "    for i, layer_size in enumerate(layer_sizes):\n",
        "        layer_top = v_spacing * (layer_size - 1) / 2\n",
        "        for j in range(layer_size):\n",
        "            pos[f'{i}-{j}'] = (i, layer_top - v_spacing * j)\n",
        "            G.add_node(f'{i}-{j}')\n",
        "    \n",
        "    # Creazione degli archi\n",
        "    for i, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
        "        for j in range(layer_size_a):\n",
        "            for k in range(layer_size_b):\n",
        "                G.add_edge(f'{i}-{j}', f'{i+1}-{k}')\n",
        "    \n",
        "    # Disegna il grafico\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    nx.draw(G, pos=pos, with_labels=False, arrows=False, node_size=300, node_color=\"lightblue\")\n",
        "    \n",
        "    # Etichette\n",
        "    for i in range(input_size):\n",
        "        pos[f'0-{i}'] = (pos[f'0-{i}'][0] - 0.1, pos[f'0-{i}'][1])\n",
        "        plt.text(pos[f'0-{i}'][0], pos[f'0-{i}'][1], f'Input {i+1}', horizontalalignment='right')\n",
        "    \n",
        "    for i in range(output_size):\n",
        "        pos[f'{n_layers-1}-{i}'] = (pos[f'{n_layers-1}-{i}'][0] + 0.1, pos[f'{n_layers-1}-{i}'][1])\n",
        "        plt.text(pos[f'{n_layers-1}-{i}'][0], pos[f'{n_layers-1}-{i}'][1], f'Output {i+1}', horizontalalignment='left')\n",
        "    \n",
        "    plt.title(\"Rappresentazione Grafica della Rete MLP\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applichiamo la funzione draw_mlp() al caso di una rete con 4 ingressi 3 strati nascosti da 3, 9 e 3 neuroni rispettivamente e 1 neurone di uscita:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametri della rete MLP utilizzata nell'esempio\n",
        "hidden_layers = (3,9, 3)  # Due strati nascosti con 10 e 5 neuroni rispettivamente\n",
        "input_size = 4  # Due caratteristiche in input\n",
        "output_size = 1  # Un neurone di output (classificazione binaria)\n",
        "\n",
        "# Disegnare la rappresentazione della rete MLP\n",
        "draw_mlp(hidden_layers, input_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Esperimento 2: Rete MLP applicata al caso di classi concentriche {#sec-lab-rete-multistrato-classi-concentriche}\n",
        "\n",
        "In questo esperimento vogliamo applicare una rete multistrato al problema della classificazione binaria nel caso di un dataset bidimensionale composto da due classi concentrichele \n",
        "La rete è composta da:\n",
        "\n",
        "- **Strato di input**: Due ingressi, ciascuno corrispondente a una delle caratteristiche del dataset (x1,x2).\n",
        "- **Strati nascosti**: Due strati nascosti, il primo con 10 neuroni e il secondo con 5 neuroni, che permettono alla rete di apprendere rappresentazioni più complesse dei dati grazie alla funzione di attivazione non lineare \"relu\" adottata. Ogni neurone in un determinato strato è connesso a tutti i neuroni dello strato successivo, consentendo il flusso delle informazioni attraverso la rete durante l'addestramento e la predizione.\n",
        "- **Strato di output**: Un singolo neurone di output, utilizzato per la classificazione binaria (classe 0, class 1).\n",
        "\n",
        "Usando la funzione introdotta nell'esempio 1 possiamo disegnare la rete MLP che vogliamo adottare per risolvere il problem di classificazione nel caso di classi concentriche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametri della rete MLP utilizzata nell'esempio\n",
        "hidden_layers = (10, 5)  # Due strati nascosti con 10 e 5 neuroni rispettivamente\n",
        "input_size = 2  # Due caratteristiche in input\n",
        "output_size = 1  # Un neurone di output (classificazione binaria)\n",
        "\n",
        "# Disegnare la rappresentazione della rete MLP\n",
        "draw_mlp(hidden_layers, input_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generazione dei dati: due classi concentriche\n",
        "def generate_concentric_circles(n_samples=500, noise=0.1):\n",
        "    np.random.seed(42)\n",
        "    n_samples_per_class = n_samples // 2\n",
        "    angles = np.random.rand(n_samples_per_class) * 2 * np.pi\n",
        "\n",
        "    inner_radius = 1 + noise * np.random.randn(n_samples_per_class)\n",
        "    outer_radius = 3 + noise * np.random.randn(n_samples_per_class)\n",
        "\n",
        "    inner_x = np.stack([inner_radius * np.cos(angles), inner_radius * np.sin(angles)], axis=1)\n",
        "    outer_x = np.stack([outer_radius * np.cos(angles), outer_radius * np.sin(angles)], axis=1)\n",
        "\n",
        "    X = np.concatenate([inner_x, outer_x], axis=0)\n",
        "    y = np.array([0] * n_samples_per_class + [1] * n_samples_per_class)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_concentric_circles()\n",
        "\n",
        "# 2. Visualizzazione dei dati\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', label='Classe 0')\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='green', label='Classe 1')\n",
        "plt.title('Dati con Classi Concentriche')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. Divisione del dataset e normalizzazione\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Creazione e addestramento del modello MLP\n",
        "model = MLPClassifier(hidden_layer_sizes=(10, 5), activation='relu', max_iter=1000, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Valutazione: matrice di confusione\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Classe 0\", \"Classe 1\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matrice di Confusione\")\n",
        "plt.show()\n",
        "\n",
        "# 6. Grafico della superficie di decisione\n",
        "h = .02  # step size\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "grid_scaled = scaler.transform(grid)\n",
        "Z = model.predict(grid_scaled)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel2, alpha=0.8)\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', label='Classe 0', edgecolors='k')\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='green', label='Classe 1', edgecolors='k')\n",
        "plt.title(\"Superficie di Decisione del Modello MLP\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il codice Python scritto per questo esperimento segue la seguente logica:\n",
        "\n",
        "- **Generazione dei dati**: Due insiemi di punti sono distribuiti in cerchi concentrici: il primo (classe 0) vicino all'origine, il secondo (classe 1) su un raggio maggiore. La forma dei dati rende il problema non linearmente separabile.\n",
        "\n",
        "- **Visualizzazione**: Il primo grafico mostra chiaramente la distribuzione circolare dei due insiemi di punti.\n",
        "\n",
        "- **Preprocessing**: I dati vengono suddivisi in un training e test set (70% - 30%) e normalizzati con StandardScaler.\n",
        "\n",
        "- **Modello MLP**: È stata creata una rete con due strati nascosti (10 e 5 neuroni) e funzione di attivazione ReLU. La rete viene addestrata per classificare i dati.\n",
        "\n",
        "- **Valutazione**: Viene calcolata e mostrata la matrice di confusione, che evidenzia l'accuratezza del modello nel distinguere le due classi.\n",
        "\n",
        "- S**uperficie di decisione**: Il terzo grafico mostra come la rete ha appreso a separare le due classi: la forma curva della regione di decisione indica che il modello ha effettivamente appreso la complessità dei dati, superando i limiti del percettrone semplice (che può solo separare linearmente).\n",
        "\n",
        "### Esperimento 3: Rete MLP per la predizione dell'esito di un caso giudiziario\n",
        "\n",
        "Applicazione di una rete neurale multistrato (MLP) per la predizione dell'esito di un caso giudiziario basandosi su tre caratteristiche: complessità del caso, esperienza dell'avvocato, e importanza mediatica.\n",
        "La rete è composta da:\n",
        "\n",
        "- **Strato di input**: Tre ingressi, ciascuno corrispondente a una delle caratteristiche del dataset (complessità del caso, esperienza dell'avvocato, importanza mediatica).\n",
        "- **Strati nascosti**: Due strati nascosti, il primo con 10 neuroni e il secondo con 5 neuroni, che permettono alla rete di apprendere rappresentazioni più complesse dei dati.\n",
        "- **Strato di output**: Un singolo neurone di output, utilizzato per la classificazione binaria (vittoria o sconfitta del caso).\n",
        "\n",
        "Usando la funzione introdotta nell'esempio 1 possiamo disegnare la rete MLP che vogliamo adottare per risolvere il problem di classificazione in studio.\n",
        "Si noti che è necessario eseguire il codice dell'esempio 1 per poter eseguire il seguente codice altrimenti Python segnalerà come errore il fatto di non conoscere la funzione draw_mlp()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametri della rete MLP utilizzata nell'esempio\n",
        "hidden_layers = (10, 5)  # Due strati nascosti con 10 e 5 neuroni rispettivamente\n",
        "input_size = 3  # Tre caratteristiche in input\n",
        "output_size = 1  # Un neurone di output (classificazione binaria)\n",
        "\n",
        "# Disegnare la rappresentazione della rete MLP\n",
        "draw_mlp(hidden_layers, input_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'implementazione in Python della rete MLP per il nostro problema di classificazione è la seguente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.preprocessing import StandardScaler # Aggiunto import\n",
        "\n",
        "# Simuliamo un dataset per predire se un caso giudiziario sarà vinto o perso basandosi su tre caratteristiche\n",
        "# Ad esempio, complessità del caso, esperienza dell'avvocato, e importanza mediatica\n",
        "\n",
        "# Generare dati di esempio\n",
        "X, y = make_classification(n_samples=200, n_features=3, n_informative=3, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Dividere i dati in train e test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --> Aggiunta sezione per lo scaling\n",
        "# Scalare i dati (buona pratica per MLP)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# <-- Fine sezione scaling\n",
        "\n",
        "# Creare e addestrare un modello MLP usando i dati scalati\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)\n",
        "mlp_model.fit(X_train_scaled, y_train) # Usa X_train_scaled\n",
        "\n",
        "# Predire sul set di test scalato\n",
        "y_pred = mlp_model.predict(X_test_scaled) # Usa X_test_scaled\n",
        "\n",
        "# Mostrare la matrice di confusione usando i dati scalati\n",
        "ConfusionMatrixDisplay.from_estimator(mlp_model, X_test_scaled, y_test, display_labels=[\"Perso\", \"Vinto\"], cmap=plt.cm.Blues) # Usa X_test_scaled\n",
        "plt.title('Matrice di Confusione')\n",
        "plt.show()\n",
        "\n",
        "# Visualizzare il rapporto di classificazione\n",
        "report = classification_report(y_test, y_pred, target_names=[\"Perso\", \"Vinto\"])\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analisi dei Risultati**\n",
        "\n",
        "1. **Matrice di Confusione**: La matrice di confusione mostra le prestazioni del modello nella classificazione dei casi giudiziari come \"Vinto\" o \"Perso\". Nel set di test, il modello ha classificato correttamente la maggior parte dei casi, con solo pochi errori. La matrice di confusione indica che il modello ha identificato con una buona precisione sia i casi vinti che quelli persi.\n",
        "\n",
        "2. **Rapporto di Classificazione**: \n",
        "   - **Precisione**: La precisione per i casi persi è del 97%, mentre per i casi vinti è dell'100%. Questo significa che quando il modello prevede un caso come \"Perso\", nel 97% dei casi ha ragione, mentre per i casi \"Vinto\", la precisione è del 100%.\n",
        "   - **Recall**: La recall per i casi persi è del 100% e per i casi vinti è del 96%. Questo indica che il modello è riuscito a identificare correttamente la quasi totalità dei casi vinti e persi.\n",
        "   - **F1-score**: L'F1-score, che rappresenta un bilanciamento tra precisione e recall, è del 99% per i casi persi e del 98% per i casi vinti, riflettendo una ottima performance complessiva del modello.\n",
        "\n",
        "**Osservazioni**: \n",
        "   - Il modello ha raggiunto un'accuratezza complessiva del 98%, che è un buon risultato considerando che i dati generati non sono perfettamente separabili.\n",
        "   - È importante notare che il modello non ha raggiunto il valore di convergenza entro il numero massimo di iterazioni impostato (1000), come indicato dall'avviso di convergenza. Questo suggerisce che con ulteriori iterazioni o con l'ottimizzazione dei parametri del modello, le prestazioni potrebbero migliorare ulteriormente.\n",
        "   - In sintesi, l'MLP si è dimostrato efficace nel classificare correttamente i casi giudiziari in base alle caratteristiche fornite, anche in presenza di dati non perfettamente distinti. \n",
        "   - Questo esempio mostra il potenziale delle reti neurali multistrato per applicazioni giuridiche, come la predizione degli esiti legali, pur sottolineando l'importanza di una corretta configurazione e addestramento del modello per ottenere i migliori risultati possibili."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "C:\\Users\\lcapitanio\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
