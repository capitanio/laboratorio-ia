{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apprendimento Non Supervisionato\n",
        "\n",
        "## Laboratorio Python\n",
        "\n",
        "### Esperimento 1: Segmentazione di documenti legali con K-means\n",
        "\n",
        "In questo esperimento Python, utilizziamo l'algoritmo K-means per raggruppare documenti legali basandoci sulla frequenza delle parole chiave presenti nei documenti.\n",
        "Si ipotizza che i documenti legali possano appartenere a categorie specifiche, come Contratti, Reati, Proprietà Privata, Assicurazioni, e Contratti di Lavoro. In particolare, si ipotizza che siano presenti 3 categorie principali. Il numero di cluster (K) è impostato a 3.\n",
        "\n",
        "Il funzionamento del semplice codice che segue può essere descritto come segue:\n",
        "\n",
        "1. Si caricano le stop word della lingua italiana in modo che il codice possa rimuovere correttamente le parole comuni in italiano come \"il\", \"di\", \"la\", migliorando la qualità del clustering K Stop Words Italiane;\n",
        "2. I documenti legali sono trasformati in vettori numerici usando la tecnica **TF-IDF**, che calcola l'importanza delle parole in ciascun documento.\n",
        "3. L'algoritmo K-means raggruppa i documenti in 3 cluster (ad esempio, Contratti, Reati, Proprietà Privata).\n",
        "4. Ogni documento viene assegnato a un cluster in base alla somiglianza del suo contenuto con altri documenti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Scarica le stop words italiane\n",
        "nltk.download('stopwords')\n",
        "stop_words_italiane = stopwords.words('italian')\n",
        "\n",
        "# Dati simulati: estratti di documenti legali\n",
        "documenti_legali = [\n",
        "    \"Il contratto di locazione è regolato dal Codice Civile.\",\n",
        "    \"Il reato di furto è punito secondo l'articolo 624.\",\n",
        "    \"La proprietà privata è garantita dalla Costituzione.\",\n",
        "    \"L'assicurazione copre i danni derivanti da incidenti stradali.\",\n",
        "    \"Il contratto di lavoro subordinato deve rispettare le norme vigenti.\",\n",
        "    \"Il Codice Penale stabilisce le pene per i reati contro la persona. Le pene sono determinate dalla gravità del reato.\",\n",
        "    \"Le controversie contrattuali sono risolte tramite arbitrato.\",\n",
        "]\n",
        "\n",
        "# Trasformazione dei documenti in rappresentazioni numeriche (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words_italiane)\n",
        "X = vectorizer.fit_transform(documenti_legali)\n",
        "\n",
        "# Applicazione di K-means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Etichette dei cluster\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Visualizzazione dei risultati\n",
        "for i, documento in enumerate(documenti_legali):\n",
        "    print(f\"Documento: {documento}\")\n",
        "    print(f\"Appartiene al cluster: {labels[i]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Esperimento 2: Identificazione di anomalie nei dati giudiziari con DBSCAN\n",
        "\n",
        "In questo epserimento useremo l'algoritmo DBSCAN per analizzare un dataset multivariato simulato in ambito giuridico. L'obiettivo è individuare pattern o anomalie nei dati relativi a sentenze legali.\n",
        "Il funzionamento del codice che segue può essere descritto come segue:\n",
        "\n",
        "1. Viene generato un dataset simulato contenente variabili come numero di testimoni, durata dei processi, gravità del reato, ecc.\n",
        "2. L'algoritmo DBSCAN viene applicato per identificare cluster di casi simili e individuare eventuali anomalie o outlier. DBSCAN assegna il valore `-1` ai punti considerati outlier.\n",
        "3. I risultati vengono visualizzati graficamente in 2D (numero di testimoni contro durata dei processi), con cluster distinti identificati da colori diversi, per una comprensione visiva dei cluster e delle anomalie. In particolare, il cluster -1 è quello composto dalle anomalie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generazione di dati simulati: caratteristiche di sentenze legali\n",
        "np.random.seed(42)\n",
        "\n",
        "# Creazione di variabili\n",
        "numero_testimoni = np.random.randint(1, 10, size=200)  # Numero di testimoni per caso\n",
        "durata_processi = np.random.normal(loc=12, scale=4, size=200)  # Durata dei processi in mesi\n",
        "gravita_reato = np.random.randint(1, 5, size=200)  # Gravità del reato (1=leggera, 4=grave)\n",
        "\n",
        "# Creazione di outlier\n",
        "outlier_testimoni = [20, 25]\n",
        "outlier_durata = [40, 50]\n",
        "outlier_gravita = [5, 5]\n",
        "\n",
        "# Inserimento degli outlier nei dati\n",
        "numero_testimoni = np.append(numero_testimoni, outlier_testimoni)\n",
        "durata_processi = np.append(durata_processi, outlier_durata)\n",
        "gravita_reato = np.append(gravita_reato, outlier_gravita)\n",
        "\n",
        "# Combinazione delle variabili in un DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Numero_Testimoni': numero_testimoni,\n",
        "    'Durata_Processi': durata_processi,\n",
        "    'Gravita_Reato': gravita_reato\n",
        "})\n",
        "\n",
        "# Standardizzazione dei dati\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Applicazione dell'algoritmo DBSCAN\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)  # Parametri regolabili\n",
        "labels = dbscan.fit_predict(data_scaled)\n",
        "\n",
        "# Aggiunta delle etichette al DataFrame\n",
        "data['Cluster'] = labels\n",
        "\n",
        "# Visualizzazione dei risultati\n",
        "print(\"Esempio di cluster individuati:\")\n",
        "print(data.head())\n",
        "\n",
        "# Visualizzazione in 2D (numero testimoni vs durata processi)\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster in np.unique(labels):\n",
        "    cluster_data = data[data['Cluster'] == cluster]\n",
        "    plt.scatter(cluster_data['Numero_Testimoni'], cluster_data['Durata_Processi'], label=f\"Cluster {cluster}\", alpha=0.7)\n",
        "\n",
        "plt.title(\"Cluster di Dati Legali (Numero Testimoni vs Durata Processi)\")\n",
        "plt.xlabel(\"Numero Testimoni\")\n",
        "plt.ylabel(\"Durata Processi (mesi)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con questo approccio, è possibile identificare sia i cluster naturali nei dati sia eventuali anomalie (outlier), che potrebbero rappresentare sentenze particolarmente atipiche o significative."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "C:\\Users\\lcapitanio\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
